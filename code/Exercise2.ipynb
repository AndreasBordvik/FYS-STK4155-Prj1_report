{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2: Bias-variance trade-off and resampling techniques "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Overview of nature of the problem: n\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import sklearn.linear_model as lm\r\n",
    "from sklearn.preprocessing import PolynomialFeatures\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.pipeline import make_pipeline\r\n",
    "from sklearn.utils import resample\r\n",
    "from common import *\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "\r\n",
    "np.random.seed(SEED_VALUE)\r\n",
    "maxdegree = 15\r\n",
    "n_bootstraps = 40\r\n",
    "n = 20\r\n",
    "x = np.sort(np.random.uniform(0, 1, n))\r\n",
    "y = np.sort(np.random.uniform(0, 1, n))\r\n",
    "x,y = np.meshgrid(x,y)\r\n",
    "z = FrankeFunction(x,y) + 0.2*np.random.normal(0, size = n)\r\n",
    "print(type(x))\r\n",
    "print(x.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing bootstrap function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "polydegree = np.arange(1, maxdegree+1)\r\n",
    "MSE_test, MSE_train, bias, variance = bootstrap(x, y, z, maxdegree, n_bootstraps, OLS())\r\n",
    "\r\n",
    "plt.plot(polydegree, MSE_test,\"m\", label='MSE test')\r\n",
    "plt.plot(polydegree, MSE_train,\"c\", label='MSE train')\r\n",
    "plt.plot(polydegree, bias,\"b--\", label='bias')\r\n",
    "plt.plot(polydegree, variance,\"r--\", label='Variance')\r\n",
    "#plt.plot(polydegree, bias+variance,\"g--\", label='bias+variance')\r\n",
    "\r\n",
    "plt.xlabel(\"Model complexity / Polynomial Degree\")\r\n",
    "plt.ylabel(\"Prediction Error - MSE\")\r\n",
    "plt.xticks(polydegree)\r\n",
    "plt.grid(True)\r\n",
    "plt.legend()\r\n",
    "plt.savefig(f\"{REPORT_FIGURES}{EX2}model_complexity_using_bootstrap_function.pdf\")\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "MSE_test = np.zeros(maxdegree)\r\n",
    "MSE_train = np.zeros(maxdegree)\r\n",
    "polydegree = np.arange(1, maxdegree+1)\r\n",
    "bias = np.zeros(maxdegree)\r\n",
    "variance = np.zeros(maxdegree)\r\n",
    "std = np.zeros(maxdegree)\r\n",
    "z_flat = z.ravel().reshape(-1,1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "for degree in tqdm(range(1, maxdegree+1), desc = f\"Looping through polynomials up to {maxdegree} degrees with {n_bootstraps} bootstraps: \"):\r\n",
    "    X = create_X(x, y, n=degree)\r\n",
    "\r\n",
    "    # Train test split\r\n",
    "    X_train, X_test, z_train, z_test = prepare_data(X, z_flat, test_size=0.2, shuffle=True, scale_X=True, scale_t=True, random_state=SEED_VALUE)\r\n",
    "\r\n",
    "    model = OLS()\r\n",
    "    z_hat_trains, z_hat_tests = bootstrapping(X_train, z_train, X_test, z_test, n_bootstraps, model, keep_intercept=True)\r\n",
    "\r\n",
    "    MSE_test[degree-1] = np.mean( np.mean((z_test - z_hat_tests)**2, axis=1, keepdims=True ))\r\n",
    "    MSE_train[degree-1] = np.mean( np.mean((z_train - z_hat_trains)**2, axis=1, keepdims=True ))\r\n",
    "    bias[degree-1] = np.mean( (z_test - np.mean(z_hat_tests, axis=1, keepdims=True))**2 )\r\n",
    "    variance[degree-1] = np.mean( np.var(z_hat_tests, axis=1, keepdims=True))\r\n",
    "    std[degree-1] = np.mean( np.std(z_hat_tests, axis=1, keepdims=True))\r\n",
    "\r\n",
    "plt.plot(polydegree, MSE_test,\"m\", label='MSE test')\r\n",
    "plt.plot(polydegree, MSE_train,\"c\", label='MSE train')\r\n",
    "plt.plot(polydegree, bias,\"b--\", label='bias')\r\n",
    "plt.plot(polydegree, variance,\"r--\", label='Variance')\r\n",
    "#plt.plot(polydegree, bias+variance,\"g--\", label='bias+variance')\r\n",
    "\r\n",
    "plt.xlabel(\"Model complexity / Polynomial Degree\")\r\n",
    "plt.ylabel(\"Prediction Error - MSE\")\r\n",
    "plt.xticks(polydegree)\r\n",
    "plt.grid(True)\r\n",
    "plt.legend()\r\n",
    "plt.savefig(f\"{REPORT_FIGURES}{EX2}model_complexity_using_bootstrap.pdf\")\r\n",
    "plt.show()\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.DataFrame()\r\n",
    "df[\"polynomial_deg\"] = polydegree\r\n",
    "df[\"MSE_test\"] = MSE_test\r\n",
    "df[\"MSE_std\"] = std\r\n",
    "display(df)\r\n",
    "df.to_csv(f\"{REPORT_DATA}{EX2}bootstrap_test_MSE.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "#TODO: Number of bootstraps? \r\n",
    "n_bootstraps = 40\r\n",
    "MSE_test = np.zeros(maxdegree)\r\n",
    "MSE_train = np.zeros(maxdegree)\r\n",
    "polydegree = np.zeros(maxdegree)\r\n",
    "bias = np.zeros(maxdegree)\r\n",
    "variance = np.zeros(maxdegree)\r\n",
    "#for each degree of poly:\r\n",
    "\r\n",
    "\r\n",
    "for degree in tqdm(range(maxdegree), desc = f\"Looping through polynomials up to {n} degrees with {n_bootstraps} bootstraps: \"):\r\n",
    "    #model = LinearRegression()\r\n",
    "    #model= make_pipeline(PolynomialFeatures(degree=degree), LinearRegression(fit_intercept=False)) \r\n",
    "    X = create_X(x, y, n=degree)\r\n",
    "    #print(f\"X.shape:{X.shape}\")    \r\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, z.reshape(-1,1), test_size=0.2, shuffle=True)\r\n",
    "    \r\n",
    "    X_train, X_test, y_train, y_test = prepare_data(X, z.reshape(-1,1), scale_X=False, scale_t=False, zero_center=False)\r\n",
    "\r\n",
    "    #print(z.shape)\r\n",
    "    #reshape for broadcasting in MSE_test and MSE_val  \r\n",
    "    y_test_ = np.reshape(y_test, newshape=(y_test.shape[0],1))\r\n",
    "    #reshape for broadcasting in MSE_test and MSE_val  \r\n",
    "    y_train_ = np.reshape(y_train, newshape=(y_train.shape[0],1))\r\n",
    "\r\n",
    "    #y_train = np.reshape(y_train, newshape=(y_train.shape[0],1))\r\n",
    "    #TODO: why scale?!?\r\n",
    "\r\n",
    "    # Commenting out the standard scaler, to try an implementation based on manual scaling as presented in the lecture notes\r\n",
    "    #scaler = StandardScaler()\r\n",
    "    #scaler.fit(X_train)\r\n",
    "    #X_train_scaled = scaler.transform(X_train)\r\n",
    "    #X_test_scaled = scaler.transform(X_test)\r\n",
    "    X_train = X_train - np.mean(X_train,axis=0)\r\n",
    "    X_test = X_test - np.mean(X_test,axis=0)\r\n",
    "    y_train = y_train - np.mean(y_train,axis=0)\r\n",
    "    y_test = y_test - np.mean(y_test,axis=0)\r\n",
    "    y_pred = np.empty((y_test.shape[0], n_bootstraps))\r\n",
    "    y_fit = np.empty((y_train.shape[0], n_bootstraps))\r\n",
    "    model = OLS()\r\n",
    "    \r\n",
    "    for i in range(n_bootstraps):\r\n",
    "        #bootstrap:\r\n",
    "        x_, y_ = resample(X_train, y_train)\r\n",
    "        #fit model to x_,y_ sample:\r\n",
    "        y_hat_train = model.fit(x_, y_, SVDfit=False)\r\n",
    "        t_hat_test = model.predict(X_test)\r\n",
    "        #clf = LinearRegression().fit(x_, y_)\r\n",
    "        #fit model and predict on test data:\r\n",
    "        y_pred[:, i] = y_hat_train.ravel()\r\n",
    "        #predict on train data:\r\n",
    "        y_fit[:,i] = t_hat_test.ravel()\r\n",
    "        \r\n",
    "\r\n",
    "    polydegree[degree] = degree\r\n",
    "    #print(f\"y_test.shape:{y_test.shape}, y_pred.shape{y_pred.shape}\")\r\n",
    "    MSE_test[degree] = np.mean( np.mean((y_test - y_pred)**2, axis=1, keepdims=True ))\r\n",
    "    MSE_train[degree] = np.mean( np.mean((y_train - y_fit)**2, axis=1, keepdims=True ))\r\n",
    "    bias[degree] = np.mean( (y_test - np.mean(y_pred, axis=1, keepdims=True))**2 )\r\n",
    "    variance[degree] = np.mean( np.var(y_pred, axis=1, keepdims=True))\r\n",
    "    \r\n",
    "    \r\n",
    "plt.plot(polydegree, MSE_test,\"m\", label='MSE_test')\r\n",
    "plt.plot(polydegree, MSE_train,\"c\", label='MSE_train')\r\n",
    "\r\n",
    "plt.plot(polydegree, bias,\"b--\", label='bias')\r\n",
    "plt.plot(polydegree, variance,\"r--\", label='Variance')\r\n",
    "#plt.plot(polydegree, bias+variance,\"g--\", label='bias+variance')\r\n",
    "\r\n",
    "plt.xlabel(\"Model complexity / Polynomial Degree\")\r\n",
    "plt.ylabel(\"Prediction Error\")\r\n",
    "\r\n",
    "plt.grid(True)\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Studying bias-variance tradeoff as dependance on the number of datpoints\n",
    "The following section will create datasets with an increasing number of datapoints. The purpouse of this analysis is to study how the bias-variance tradeoff for Ordinary Least squares varies as a function of the number of datapoints. This is an addendum to the previous analysis where we studied the bias-variance tradeoff as a function of the model complexity. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "\r\n",
    "n_list = [10, 20, 30, 40, 50]\r\n",
    "n_bootstraps = 20\r\n",
    "maxdegree = 12\r\n",
    "polydegree = np.arange(maxdegree)\r\n",
    "\r\n",
    "for n in n_list:\r\n",
    "    x = np.sort(np.random.uniform(0,1,n))\r\n",
    "    y = np.sort(np.random.uniform(0,1,n))\r\n",
    "    x,y = np.meshgrid(x,y)\r\n",
    "    z = FrankeFunction(x, y) + 0.2*np.random.normal(0, size = n)\r\n",
    "\r\n",
    "    MSE_test_n = np.zeros(maxdegree)\r\n",
    "    MSE_train_n = np.zeros(maxdegree)\r\n",
    "    variance_n = np.zeros(maxdegree)\r\n",
    "    bias_n = np.zeros(maxdegree)\r\n",
    "\r\n",
    "    for degree in tqdm(range(maxdegree), desc = f\"Looping through polynomials up to {maxdegree} degrees with {n_bootstraps} bootstraps: \"):\r\n",
    "        model = LinearRegression()\r\n",
    "        #model= make_pipeline(PolynomialFeatures(degree=degree), LinearRegression(fit_intercept=False)) \r\n",
    "        X = create_X(x, y, n=degree)    \r\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, z.reshape(-1,1), test_size=0.2)\r\n",
    "\r\n",
    "        #reshape for broadcasting in MSE_test and MSE_val  \r\n",
    "        y_test_ = np.reshape(y_test, newshape=(y_test.shape[0],1))\r\n",
    "        #reshape for broadcasting in MSE_test and MSE_val  \r\n",
    "        y_train_ = np.reshape(y_train, newshape=(y_train.shape[0],1))\r\n",
    "\r\n",
    "        #y_train = np.reshape(y_train, newshape=(y_train.shape[0],1))\r\n",
    "        #TODO: why scale?!?\r\n",
    "\r\n",
    "        scaler = StandardScaler()\r\n",
    "        scaler.fit(X_train)\r\n",
    "        X_train_scaled = scaler.transform(X_train)\r\n",
    "        X_test_scaled = scaler.transform(X_test)\r\n",
    "        y_pred = np.empty((y_test.shape[0], n_bootstraps))\r\n",
    "        y_fit = np.empty((y_train.shape[0], n_bootstraps))\r\n",
    "    \r\n",
    "        for i in range(n_bootstraps):\r\n",
    "            #bootstrap:\r\n",
    "            x_, y_ = resample(X_train_scaled, y_train)\r\n",
    "            #fit model to x_,y_ sample:\r\n",
    "            #print(f\"y_.shape : {y_.shape}\")\r\n",
    "            model.fit(x_, y_, SVDfit=False)\r\n",
    "            #fit model and predict on test data:\r\n",
    "            y_pred[:, i] = model.predict(X_test_scaled).ravel()\r\n",
    "            #predict on train data:\r\n",
    "            y_fit[:,i] = model.predict(X_train_scaled).ravel()\r\n",
    "        \r\n",
    "\r\n",
    "        #polydegree[degree] = degree\r\n",
    "   \r\n",
    "        MSE_test_n[degree] = np.mean( np.mean((y_test_ - y_pred)**2, axis=1, keepdims=True) )\r\n",
    "        MSE_train_n[degree] = np.mean( np.mean((y_train_ - y_fit)**2, axis=1, keepdims=True) )\r\n",
    "        bias_n[degree] = np.mean( (y_test - np.mean(y_pred, axis=1, keepdims=True))**2 )\r\n",
    "        variance_n[degree] = np.mean( np.var(y_pred, axis=1, keepdims=True))\r\n",
    "    \r\n",
    "    plt.plot(polydegree, MSE_test_n,\"m\", label='MSE_test')\r\n",
    "    plt.plot(polydegree, MSE_train_n,\"c\", label='MSE_train')\r\n",
    "\r\n",
    "    plt.plot(polydegree, bias_n,\"g--\", label='bias')\r\n",
    "    plt.plot(polydegree, variance_n,\"r--\", label='Variance')\r\n",
    "\r\n",
    "    plt.title(f\"Bias-Variance tradeoff for n={n} datapoints\")\r\n",
    "    plt.xlabel(\"Model complexity / Polynomial Degree\")\r\n",
    "    plt.ylabel(\"Prediction Error\")\r\n",
    "\r\n",
    "    plt.grid(True)\r\n",
    "    plt.legend()\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "    \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('pytorch1': conda)"
  },
  "interpreter": {
   "hash": "4100b45d1ec7c24b8fe1569d39871ffb9fcc213dcc50046fc68fe247fbf6e84f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}