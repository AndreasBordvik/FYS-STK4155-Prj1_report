{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 2 - Ridge Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from common import *\n",
    "import pandas as pd\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "import sklearn.linear_model as lm\n",
    "\n",
    "print(f\"Root directory: {os.getcwd()}\")\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "    \"font.size\": 10,\n",
    "})\n",
    "\n",
    "%matplotlib inline "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Data\n",
    "Defining and creating the data\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features = 6\n",
    "x = np.arange(0, 1, 0.05)\n",
    "y = np.arange(0, 1, 0.05)\n",
    "X = create_X(x, y, n=features) # Design Matrix\n",
    "z = FrankeFunction(x,y)\n",
    "# z += noise # Adding stochastic noise"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 - Finding optimal lambda for Ridge fit\n",
    "First we find the optimal value for the lambda parameter by splitting the input data into training and test data. Than we fit the model using different values for lambda. For each lambda value we use the mean square error on the test data evaluate how god the fit is. The best lambda value is used to fit a new model with all data (not using train-test split). <br>\n",
    "First we find the optimal parameter value of lambda, $\\lambda$, by evaluating overfit from evaluation plot where we plot the values for MSE for training and test data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nbf_lambdas = 10\n",
    "lambdas = np.logspace(-5,5, nbf_lambdas)\n",
    "z_train_ridge = pd.DataFrame()\n",
    "z_hat_train_ridge = pd.DataFrame()\n",
    "z_test_ridge = pd.DataFrame()\n",
    "z_hat_test_ridge = pd.DataFrame()\n",
    "\n",
    "\n",
    "z_hat_train_ridge_sk = pd.DataFrame()\n",
    "z_hat_test_ridge_sk = pd.DataFrame()\n",
    "\n",
    "for lmb in lambdas:\n",
    "    model_sk = lm.Ridge(lmb, fit_intercept=False)\n",
    "    #X_train, X_test, z_train, z_test = prepare_data(X, z, test_size=0.2, shuffle=True, scale_X=False, scale_t=False)\n",
    "    X_train, X_test, z_train, z_test = prepare_data(X, z, test_size=0.2, shuffle=True, scale_X=False, scale_t=False, random_state=4155)\n",
    "    model = RidgeRegression(lmb) # The model\n",
    "    model.fit(X_train, z_train) # Fitting the model\n",
    "\n",
    "    # Fitting the scikit Ridge model\n",
    "    model_sk.fit(X_train, z_train)\n",
    "   \n",
    "    # Predictions our Ridge model\n",
    "    z_hat_train = model.predict(X_train) # predict on train data\n",
    "    z_hat_test = model.predict(X_test) # predict on test data\n",
    "\n",
    "    # Predictions sklearn Ridge\n",
    "    z_hat_train_sk = model_sk.predict(X_train) # predict on train data\n",
    "    z_hat_test_sk = model_sk.predict(X_test) # predict on test data\n",
    "\n",
    "    # Filling up dataframes\n",
    "    z_train_ridge[lmb] = z_train.flatten() \n",
    "    z_hat_train_ridge[lmb] = z_hat_train.flatten()\n",
    "    z_test_ridge[lmb] = z_test.flatten()\n",
    "    z_hat_test_ridge[lmb] = z_hat_test.flatten()\n",
    "\n",
    "    # Filling up sk dataframes\n",
    "    z_hat_train_ridge_sk[lmb] = z_hat_train_sk.flatten()\n",
    "    z_hat_test_ridge_sk[lmb] = z_hat_test_sk.flatten()\n",
    "\n",
    "\n",
    "# MSE calculations for all lambda values\n",
    "mse_scores_train = ((z_train_ridge - z_hat_train_ridge) ** 2).mean()\n",
    "mse_scores_test = ((z_test_ridge - z_hat_test_ridge) ** 2).mean()\n",
    "mse_scors_train_sk = ((z_train_ridge - z_hat_train_ridge_sk) ** 2).mean()\n",
    "mse_scores_test_sk = ((z_test_ridge - z_hat_test_ridge_sk) ** 2).mean()\n",
    "\n",
    "# R2 calculations for all lambda values\n",
    "R2_scores_train = 1 - ((z_train_ridge - z_hat_train_ridge) ** 2).sum() / ((z_train_ridge - z_train_ridge.mean())**2).sum() \n",
    "R2_scores_test = 1 - ((z_test_ridge - z_hat_test_ridge) ** 2).sum() / ((z_test_ridge - z_test_ridge.mean())**2).sum()  \n",
    "\n",
    "# Plots\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(-np.log(lambdas), mse_scores_train, label=\"Training data\")\n",
    "plt.plot(-np.log(lambdas), mse_scores_test, label=\"Test data\")\n",
    "\n",
    "plt.plot(-np.log(lambdas), mse_scors_train_sk, 'm--', label=\"Training data sklearn\")\n",
    "plt.plot(-np.log(lambdas), mse_scores_test_sk, 'y--', label=\"Test data sklearn\")\n",
    "\n",
    "#plt.plot(mse_scores_train,-np.log(lambdas), label=\"Training data\")\n",
    "#plt.plot(mse_scores_test,-np.log(lambdas), label=\"Test data\")\n",
    "plt.xlabel(\"log(lambda)\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Training evaluation on Ridge regression fit\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{REPORT_FIGURES}franke_function_Ridge_evaluate_fit.pdf\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform the same analysis as above, but with scaled data and fit intercept equal to false"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nbf_lambdas = 10\n",
    "lambdas = np.logspace(-5,5, nbf_lambdas)\n",
    "z_train_ridge = pd.DataFrame()\n",
    "z_hat_train_ridge = pd.DataFrame()\n",
    "z_test_ridge = pd.DataFrame()\n",
    "z_hat_test_ridge = pd.DataFrame()\n",
    "\n",
    "\n",
    "z_hat_train_ridge_sk = pd.DataFrame()\n",
    "z_hat_test_ridge_sk = pd.DataFrame()\n",
    "\n",
    "for lmb in lambdas:\n",
    "    model_sk = lm.Ridge(lmb, fit_intercept=False)\n",
    "    #X_train, X_test, z_train, z_test = prepare_data(X, z, test_size=0.2, shuffle=True, scale_X=False, scale_t=False)\n",
    "    X_train, X_test, z_train, z_test = prepare_data(X, z, test_size=0.2, shuffle=True, scale_X=False, scale_t=False, random_state=4155)\n",
    "    model = RidgeRegression(lmb) # The model\n",
    "    model.fit(X_train, z_train) # Fitting the model\n",
    "\n",
    "    # Fitting the scikit Ridge model\n",
    "    model_sk.fit(X_train, z_train)\n",
    "   \n",
    "    # Predictions our Ridge model\n",
    "    z_hat_train = model.predict(X_train) # predict on train data\n",
    "    z_hat_test = model.predict(X_test) # predict on test data\n",
    "\n",
    "    # Predictions sklearn Ridge\n",
    "    z_hat_train_sk = model_sk.predict(X_train) # predict on train data\n",
    "    z_hat_test_sk = model_sk.predict(X_test) # predict on test data\n",
    "\n",
    "    # Filling up dataframes\n",
    "    z_train_ridge[lmb] = z_train.flatten() \n",
    "    z_hat_train_ridge[lmb] = z_hat_train.flatten()\n",
    "    z_test_ridge[lmb] = z_test.flatten()\n",
    "    z_hat_test_ridge[lmb] = z_hat_test.flatten()\n",
    "\n",
    "    # Filling up sk dataframes\n",
    "    z_hat_train_ridge_sk[lmb] = z_hat_train_sk.flatten()\n",
    "    z_hat_test_ridge_sk[lmb] = z_hat_test_sk.flatten()\n",
    "\n",
    "\n",
    "# MSE calculations for all lambda values\n",
    "mse_scores_train = ((z_train_ridge - z_hat_train_ridge) ** 2).mean()\n",
    "mse_scores_test = ((z_test_ridge - z_hat_test_ridge) ** 2).mean()\n",
    "mse_scors_train_sk = ((z_train_ridge - z_hat_train_ridge_sk) ** 2).mean()\n",
    "mse_scores_test_sk = ((z_test_ridge - z_hat_test_ridge_sk) ** 2).mean()\n",
    "\n",
    "# R2 calculations for all lambda values\n",
    "R2_scores_train = 1 - ((z_train_ridge - z_hat_train_ridge) ** 2).sum() / ((z_train_ridge - z_train_ridge.mean())**2).sum() \n",
    "R2_scores_test = 1 - ((z_test_ridge - z_hat_test_ridge) ** 2).sum() / ((z_test_ridge - z_test_ridge.mean())**2).sum()  \n",
    "\n",
    "# Plots\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(-np.log(lambdas), mse_scores_train, label=\"Training data\")\n",
    "plt.plot(-np.log(lambdas), mse_scores_test, label=\"Test data\")\n",
    "\n",
    "plt.plot(-np.log(lambdas), mse_scors_train_sk, 'm--', label=\"Training data sklearn\")\n",
    "plt.plot(-np.log(lambdas), mse_scores_test_sk, 'y--', label=\"Test data sklearn\")\n",
    "\n",
    "#plt.plot(mse_scores_train,-np.log(lambdas), label=\"Training data\")\n",
    "#plt.plot(mse_scores_test,-np.log(lambdas), label=\"Test data\")\n",
    "plt.xlabel(\"log(lambda)\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Training evaluation on Ridge regression fit\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{REPORT_FIGURES}franke_function_Ridge_evaluate_fit.pdf\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.3.1\n",
    "Finding the optimal lambda by using the bootstrap resampling technique over different values of lamda. For each value of lambda, a bias-variance tradeoff chart as in exercise 2 is created. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_bootstraps = 100\n",
    "maxdegree = 12\n",
    "for lmb in lambdas:\n",
    "    MSE_test = np.zeros(maxdegree)\n",
    "    MSE_train = np.zeros(maxdegree)\n",
    "    polydegree = np.arange(maxdegree)\n",
    "    bias = np.zeros(maxdegree)\n",
    "    variance = np.zeros(maxdegree)\n",
    "\n",
    "    for degree in tqdm(range(maxdegree), desc = f\"Looping through polynomials up to {maxdegree} degrees with {n_bootstraps} bootstraps: \"):\n",
    "        model = RidgeRegression(lmb)\n",
    "        X = create_X(x, y, n=degree)\n",
    "        X_train, X_test, z_train, z_test = prepare_data(X, z)\n",
    "\n",
    "        # Reshape for broadcasting\n",
    "        z_test_ = np.reshape(z_test, newshape=(z_test.shape[0],1))\n",
    "        z_train_ = np.reshape(z_train, newshape=(z_train.shape[0],1))\n",
    "        \n",
    "        # Scaling data and preparing output arrays \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        #X_train_scaled = scaler.transform(X_train)\n",
    "        #X_test_scaled = scaler.transform(X_test)\n",
    "        z_pred = np.empty((z_test.shape[0], n_bootstraps))\n",
    "        z_fit = np.empty((z_train.shape[0], n_bootstraps))\n",
    "\n",
    "        # Bootstrapping\n",
    "        for i in range(n_bootstraps):\n",
    "            x_, z_ = resample(X_train, z_train)\n",
    "            model.fit(X_train, z_train)\n",
    "            z_pred[:,i] = model.predict(X_test)\n",
    "            z_fit[:,i] = model.predict(X_train)\n",
    "\n",
    "        MSE_test[degree] = np.mean( np.mean((z_test_ - z_pred)**2, axis=1, keepdims=True) )\n",
    "        MSE_train[degree] = np.mean( np.mean((z_train_ - z_fit)**2, axis=1, keepdims=True) )\n",
    "        bias[degree] = np.mean( (z_test - np.mean(z_pred, axis=1, keepdims=True))**2 )\n",
    "        variance[degree] = np.mean( np.var(z_pred, axis=1, keepdims=True))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(polydegree, MSE_test,\"m\", label='MSE\\_test')\n",
    "    plt.plot(polydegree, MSE_train,\"c\", label='MSE\\_train')\n",
    "\n",
    "    plt.plot(polydegree, bias,\"g--\", label='bias')\n",
    "    plt.plot(polydegree, variance,\"r--\", label='Variance')\n",
    "\n",
    "    plt.title(f\"Bias-Variance for log10(lambda) = {np.log10(lmb)}\")\n",
    "    plt.xlabel(\"Model complexity / Polynomial Degree\")\n",
    "    plt.ylabel(\"Prediction Error\")\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    display(model.summary())\n",
    "      \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 Analysis of plots and training metrics\n",
    "Do the analysis.......\n",
    "\n",
    "Conclusion:<br>\n",
    "Based on the analysis, we conclude that a model complexity of degree 5 yields the most optimal fit."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5 plot of the model using the most optimal parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scale = False\n",
    "\n",
    "if scale:\n",
    "    # Data Scalling\n",
    "    X_scaler = StandardScaler()\n",
    "    X_scaler.fit(X)\n",
    "    X_train = X_scaler.transform(X_train)\n",
    "    z_train = np.expand_dims(z,axis=1)  \n",
    "    z_scaler = StandardScaler()\n",
    "    z_scaler.fit(z_train)\n",
    "    z_train = z_scaler.transform(z_train)\n",
    "else:\n",
    "    X_train = X; z_train = z\n",
    "\n",
    "lam = -1.1\n",
    "model = RidgeRegression(lam) # The model\n",
    "model.fit(X_train, z_train) # Fitting the model\n",
    "z_hat = model.predict(X_train) # predict on train data\n",
    "\n",
    "# Evaluatation metrics\n",
    "# TODO:\n",
    "results_df = pd.DataFrame(columns=[\"MSE\", \"R2-score\"], index=[\"Training data\", \"Test data\"])\n",
    "results_df[\"MSE\"] = MSE(z, z_hat)\n",
    "results_df[\"R2-score\"] = R2(z, z_hat)\n",
    "results_df.to_csv(f\"{REPORT_DATA}redge_reg_lambda_{lam}.csv\")\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure()#figsize=(8,8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.title.set_text(f\"Ridge regression fit to the Franke Function\\nDegree {degree},$\\lambda$:{lam}\")\n",
    "#ax.view_init(elev=5., azim=85.0)\n",
    "ax.view_init(elev=30., azim=-25.0)\n",
    "ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_zlabel(\"z\")\n",
    "ax.scatter3D(x,y,z,c=z, cmap=cm.coolwarm, marker = '.')\n",
    "plt.savefig(f\"{REPORT_FIGURES}franke_function_Ridge_best_fit.pdf\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}