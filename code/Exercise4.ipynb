{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 2 - Ridge Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from common import *\n",
    "import pandas as pd\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "\n",
    "os.chdir(\"../\")\n",
    "print(f\"Root directory: {os.getcwd()}\")\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "    \"font.size\": 10,\n",
    "})\n",
    "\n",
    "%matplotlib inline "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Root directory: /Users/gardpavels/uio/fysstk_proj1/FYS-STK4155-Prj1_report\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "INPUT_DATA = \"data/input_data/\"  # Path for input data\n",
    "REPORT_DATA = \"data/report_data/\" # Path for data ment for the report\n",
    "REPORT_FIGURES = \"figures/\" # Path for figures ment for the report"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Data\n",
    "Defining and creating the data\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "features = 6\n",
    "#x = np.arange(0, 1, 0.05)\n",
    "#y = np.arange(0, 1, 0.05)\n",
    "X = create_X(x, y, n=features) # Design Matrix\n",
    "z = FrankeFunction(x,y)\n",
    "z += noise # Adding stochastic noise"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-dc58acfc823d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#x = np.arange(0, 1, 0.05)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#y = np.arange(0, 1, 0.05)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Design Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrankeFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;31m# Adding stochastic noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 - Finding optimal lambda for Ridge fit\n",
    "First we find the optimal value for the lambda parameter by splitting the input data into training and test data. Than we fit the model using different values for lambda. For each lambda value we use the mean square error on the test data evaluate how god the fit is. The best lambda value is used to fit a new model with all data (not using train-test split). <br>\n",
    "First we find the optimal parameter value of lambda, $\\lambda$, by evaluating overfit from evaluation plot where we plot the values for MSE for training and test data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nbf_lambdas = 100\n",
    "lambdas = np.logspace(-6,6, nbf_lambdas)\n",
    "z_train_ridge = pd.DataFrame()\n",
    "z_hat_train_ridge = pd.DataFrame()\n",
    "z_test_ridge = pd.DataFrame()\n",
    "z_hat_test_ridge = pd.DataFrame()\n",
    "\n",
    "for lam in lambdas:\n",
    "    #X_train, X_test, z_train, z_test = prepare_data(X, z, test_size=0.2, shuffle=True, scale_X=False, scale_t=False)\n",
    "    X_train, X_test, z_train, z_test = prepare_data(X, z, test_size=0.2, shuffle=True, scale_X=False, scale_t=False, random_state=4155)\n",
    "    model = RidgeRegression(lam) # The model\n",
    "    model.fit(X_train, z_train) # Fitting the model\n",
    "   \n",
    "    # Predictions\n",
    "    z_hat_train = model.predict(X_train) # predict on train data\n",
    "    z_hat_test = model.predict(X_test) # predict on test data\n",
    "\n",
    "    # Filling up dataframes\n",
    "    z_train_ridge[lam] = z_train.flatten() \n",
    "    z_hat_train_ridge[lam] = z_hat_train.flatten()\n",
    "    z_test_ridge[lam] = z_test.flatten()\n",
    "    z_hat_test_ridge[lam] = z_hat_test.flatten()\n",
    "\n",
    "\n",
    "# MSE calculations for all lambda values\n",
    "mse_scores_train = ((z_train_ridge - z_hat_train_ridge) ** 2).mean()\n",
    "mse_scores_test = ((z_test_ridge - z_hat_test_ridge) ** 2).mean()\n",
    "# R2 calculations for all lambda values\n",
    "R2_scores_train = 1 - ((z_train_ridge - z_hat_train_ridge) ** 2).sum() / ((z_train_ridge - z_train_ridge.mean())**2).sum() \n",
    "R2_scores_test = 1 - ((z_test_ridge - z_hat_test_ridge) ** 2).sum() / ((z_test_ridge - z_test_ridge.mean())**2).sum()  \n",
    "\n",
    "# Plots\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(-np.log(lambdas), mse_scores_train, label=\"Training data\")\n",
    "plt.plot(-np.log(lambdas), mse_scores_test, label=\"Test data\")\n",
    "#plt.plot(mse_scores_train,-np.log(lambdas), label=\"Training data\")\n",
    "#plt.plot(mse_scores_test,-np.log(lambdas), label=\"Test data\")\n",
    "plt.xlabel(\"log(lambda)\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Training evaluation on Ridge regression fit\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{REPORT_FIGURES}franke_function_Ridge_evaluate_fit.pdf\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 Analysis of plots and training metrics\n",
    "Do the analysis.......\n",
    "\n",
    "Conclusion:<br>\n",
    "Based on the analysis, we conclude that a model complexity of degree 5 yields the most optimal fit."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5 plot of the model using the most optimal parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scale = False\n",
    "\n",
    "if scale:\n",
    "    # Data Scalling\n",
    "    X_scaler = StandardScaler()\n",
    "    X_scaler.fit(X)\n",
    "    X_train = X_scaler.transform(X_train)\n",
    "    z_train = np.expand_dims(z,axis=1)  \n",
    "    z_scaler = StandardScaler()\n",
    "    z_scaler.fit(z_train)\n",
    "    z_train = z_scaler.transform(z_train)\n",
    "else:\n",
    "    X_train = X; z_train = z\n",
    "\n",
    "lam = -1.1\n",
    "model = RidgeRegression(lam) # The model\n",
    "model.fit(X_train, z_train) # Fitting the model\n",
    "z_hat = model.predict(X_train) # predict on train data\n",
    "\n",
    "# Evaluatation metrics\n",
    "# TODO:\n",
    "results_df = pd.DataFrame(columns=[\"MSE\", \"R2-score\"], index=[\"Training data\", \"Test data\"])\n",
    "results_df[\"MSE\"] = MSE(z, z_hat)\n",
    "results_df[\"R2-score\"] = R2(z, z_hat)\n",
    "results_df.to_csv(f\"{REPORT_DATA}redge_reg_lambda_{lam}.csv\")\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure()#figsize=(8,8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.title.set_text(f\"Ridge regression fit to the Franke Function\\nDegree {degree},$\\lambda$:{lam}\")\n",
    "#ax.view_init(elev=5., azim=85.0)\n",
    "ax.view_init(elev=30., azim=-25.0)\n",
    "ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_zlabel(\"z\")\n",
    "ax.scatter3D(x,y,z,c=z, cmap=cm.coolwarm, marker = '.')\n",
    "plt.savefig(f\"{REPORT_FIGURES}franke_function_Ridge_best_fit.pdf\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}