{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 6: Analysis of real data  (score 30 points)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import seaborn as sns\r\n",
    "from imageio import imread\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from mpl_toolkits.mplot3d import Axes3D\r\n",
    "from matplotlib import cm\r\n",
    "import seaborn as sns\r\n",
    "import os\r\n",
    "from common import *\r\n",
    "import cv2\r\n",
    "#from mpl_toolkits.mplot3d import Axes3D\r\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "\r\n",
    "\r\n",
    "print(f\"Root directory: {os.getcwd()}\")\r\n",
    "\r\n",
    "plt.rcParams.update({\r\n",
    "    \"text.usetex\": True,\r\n",
    "    \"font.family\": \"serif\",\r\n",
    "    \"font.serif\": [\"Palatino\"],\r\n",
    "    \"font.size\": 10,\r\n",
    "})\r\n",
    "\r\n",
    "#%matplotlib inline "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Global variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.random.seed(SEED_VALUE) # Random seed to guarantee reproducibility\r\n",
    "# Paths\r\n",
    "#INPUT_DATA = \"data/input_data/\"  # Path for input data\r\n",
    "#REPORT_DATA = \"data/report_data/\" # Path for data ment for the report\r\n",
    "#REPORT_FIGURES = \"figures/\" # Path for figures ment for the report\r\n",
    "# Setting for range of degrees\r\n",
    "#from_degree = 0\r\n",
    "#to_degree = 14\r\n",
    "#degrees = np.arange(from_degree,to_degree)\r\n",
    "#degree = 5\r\n",
    "# Setting for logspace range of lambdas \r\n",
    "#from_lambda = -7 #\r\n",
    "#to_lambda = 3 #\r\n",
    "#nLambdas = 10\r\n",
    "#lambdas = np.logspace(from_lambda, to_lambda, nLambdas)\r\n",
    "# Rescale settings\r\n",
    "rescale_factor = 0.2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Reading and plotting terrain data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load the terrain\r\n",
    "terrain1_file = \"SRTM_data_Norway_1.tif\"\r\n",
    "terrain2_file = \"SRTM_data_Norway_2.tif\"\r\n",
    "terrain1 = imread(f'{INPUT_DATA}{terrain1_file}')\r\n",
    "terrain2 = imread(f'{INPUT_DATA}{terrain2_file}')\r\n",
    "\r\n",
    "# Plotting terrain\r\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\r\n",
    "ax1.title.set_text(\"Terrain over Norway 1\")\r\n",
    "ax1.set_xlabel(\"X\"); ax1.set_ylabel(\"Y\")\r\n",
    "surf1 = ax1.imshow(terrain1, cmap='gray')\r\n",
    "ax2.title.set_text(\"Terrain over Norway 2\")\r\n",
    "ax2.set_xlabel(\"X\"); ax2.set_ylabel(\"Y\")\r\n",
    "surf2 = ax2.imshow(terrain2, cmap='gray')\r\n",
    "plt.savefig(f\"{REPORT_FIGURES}terrain_data.pdf\")\r\n",
    "plt.show()\r\n",
    "print(terrain1[0,0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Thoughts on the topographical data\r\n",
    "Before we proceed with this exercise, we want to briefly discuss the nature of this terrain data compared to the generated frank function. We regard the franke function as generic because it is known and its behavior or shape can be generated for other values of x and y. Thus, creating a model that can generalize the function even with added noise for other unknown data points for x and y makes sense. However, we are uncertain if this idea of generalization is transferable to topographical terrain data in the same sense. The terrain data given is unique, and its shape cannot be generalized for unknown data points. If the purpose is to create the absolute best fit for the specific and unique terrain data, one could simply overfit to the terrain data by having an extremely high degree when fitting a model, and that is not really ML. One could probably argue that a model should tackle added noise on the terrain data and still be able to represent the shapes and contours incorporated in the image. However, images are what they are, and in this case, the topographical terrain \"is what it is.\"  With that being said, we proceed with this exercise in the same spirit as done for exercises 1-5 even though we question this use case when working on this kind of real data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3D plot of the whole Terrain image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make data for terrain1\r\n",
    "z1 = np.array(terrain1)\r\n",
    "scaler = MinMaxScaler()\r\n",
    "scaler.fit(z1)\r\n",
    "z1_scaled = scaler.transform(z1)\r\n",
    "#y1 = np.linspace(0,1,z1.shape[0])\r\n",
    "#x1 = np.linspace(0,1,z1.shape[1])\r\n",
    "y1 = np.arange(0, z1.shape[0])\r\n",
    "x1 = np.arange(0, z1.shape[1])\r\n",
    "x1_m, y1_m = np.meshgrid(x1,y1)\r\n",
    "\r\n",
    "# Make data for terrain2\r\n",
    "z2 = np.array(terrain2)\r\n",
    "scaler = MinMaxScaler()\r\n",
    "scaler.fit(z2)\r\n",
    "z2_scaled = scaler.transform(z2)\r\n",
    "#y2 = np.linspace(0,1,z2.shape[0])\r\n",
    "#x2 = np.linspace(0,1,z2.shape[1])\r\n",
    "y2 = np.arange(0, z2.shape[0])\r\n",
    "x2 = np.arange(0, z2.shape[1])\r\n",
    "x2_m, y2_m = np.meshgrid(x2,y2)\r\n",
    "\r\n",
    "#%matplotlib\r\n",
    "fig = plt.figure()\r\n",
    "ax1 = fig.add_subplot(1,2,1, projection='3d')\r\n",
    "ax1.title.set_text(\"Terrain1 plot\")\r\n",
    "ax1.set_xlabel(\"x\"); ax1.set_ylabel(\"y\"); ax1.set_zlabel(\"z\")\r\n",
    "ax1.plot_surface(x1_m, y1_m, z1, cmap=cm.coolwarm, linewidth = 0, antialiased=False)\r\n",
    "\r\n",
    "ax2 = fig.add_subplot(1,2,2, projection='3d')\r\n",
    "ax2.title.set_text(\"Terrain2 plot\")\r\n",
    "ax2.set_xlabel(\"x\"); ax2.set_ylabel(\"y\"); ax2.set_zlabel(\"z\")\r\n",
    "ax2.plot_surface(x2_m, y2_m, z2, cmap=cm.coolwarm, linewidth = 0, antialiased=False)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preprocessing and transformation of terrain data\r\n",
    "Least Square regression is not designed to tackle images directly. Thus, we must first transform the terrain data by slicing it into several bits and pieces. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Resizeing the terrain image\r\n",
    "For computational purpose, we resize the terrain image to have a resonable amount of datapoints for our least sqaure models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ySize = int(terrain1.shape[0] * rescale_factor); print(ySize)\r\n",
    "xSize = int(terrain1.shape[1] * rescale_factor); print(xSize)\r\n",
    "terrain1Resized = cv2.resize(terrain1, (xSize, ySize))\r\n",
    "terrain2Resized = cv2.resize(terrain2, (xSize, ySize))\r\n",
    "\r\n",
    "# Plotting terrain\r\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\r\n",
    "ax1.title.set_text(\"Terrain over Norway 1 (Resized)\")\r\n",
    "ax1.set_xlabel(\"X\"); ax1.set_ylabel(\"Y\")\r\n",
    "surf1 = ax1.imshow(terrain1Resized, cmap='gray')\r\n",
    "ax2.title.set_text(\"Terrain over Norway 2 (Resized)\")\r\n",
    "ax2.set_xlabel(\"X\"); ax2.set_ylabel(\"Y\")\r\n",
    "surf2 = ax2.imshow(terrain2Resized, cmap='gray')\r\n",
    "plt.savefig(f\"{REPORT_FIGURES}terrain_data.pdf\")\r\n",
    "plt.show()\r\n",
    "print(terrain1[0,0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating image patches and Terrain data selection"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Methods"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_img_patches(img, ySteps, xSteps):\r\n",
    "    patches = []\r\n",
    "    for y in range(0,img.shape[0], ySteps):\r\n",
    "        for x in range(0,img.shape[1], xSteps):\r\n",
    "            y_from = y; \r\n",
    "            y_to = y+ySteps; \r\n",
    "            x_from = x; \r\n",
    "            x_to = x+xSteps; \r\n",
    "            img_patch = img[y_from:y_to, x_from:x_to]        \r\n",
    "            patches.append(img_patch)\r\n",
    "\r\n",
    "    return patches\r\n",
    "\r\n",
    "def patches_to_img(patches, ySteps, xSteps, nYpatches, nXpatches, plotImage=False):\r\n",
    "    img = np.zeros((ySteps*nYpatches, xSteps*nXpatches))\r\n",
    "    i = 0\r\n",
    "    for y in range(0,img.shape[0], ySteps):\r\n",
    "        for x in range(0,img.shape[1], xSteps):\r\n",
    "            y_from = y; \r\n",
    "            y_to = y+ySteps; \r\n",
    "            x_from = x; \r\n",
    "            x_to = x+xSteps; \r\n",
    "            img[y_from:y_to, x_from:x_to] = patches[i]         \r\n",
    "            i += 1\r\n",
    "    \r\n",
    "    if plotImage:\r\n",
    "        plt.imshow(img, cmap='gray')\r\n",
    "        plt.title(\"Reconstructed img\")\r\n",
    "        plt.show()\r\n",
    "    return img\r\n",
    "\r\n",
    "def plotTerrainPatches(patches, nYpatches, nXpatches, plotTitle=\"Terrain patches\"):\r\n",
    "    # Plotting terrain patches\r\n",
    "    fig, ax = plt.subplots(nYpatches, nXpatches,figsize=(4,8))\r\n",
    "    i=0\r\n",
    "    for y in range(nYpatches):\r\n",
    "        for x in range(nXpatches):\r\n",
    "            ax[y,x].title.set_text(f\"Patch{i}\")\r\n",
    "            ax[y,x].set_xlabel(\"X\"); ax1.set_ylabel(\"Y\")\r\n",
    "            ax[y,x].imshow(patches[i], cmap='gray')\r\n",
    "            i+=1\r\n",
    "    \r\n",
    "    fig.suptitle(f\"{plotTitle}\") # or plt.suptitle('Main title')\r\n",
    "    plt.tight_layout()\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "def createTerrainData(terrain, includeMeshgrid=True):\r\n",
    "    z = np.array(terrain) \r\n",
    "    x = np.arange(0, z.shape[1])\r\n",
    "    y = np.arange(0, z.shape[0])\r\n",
    "    if includeMeshgrid:\r\n",
    "        x, y = np.meshgrid(x,y)\r\n",
    "    return x,y,z\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nXpatches = 2; nYpatches=4\r\n",
    "ySteps = int(terrain2Resized.shape[0] / nYpatches)\r\n",
    "xSteps = int(terrain2Resized.shape[1] / nXpatches)\r\n",
    "\r\n",
    "patches_1 = create_img_patches(terrain1Resized, ySteps, xSteps)\r\n",
    "plotTerrainPatches(patches_1, nYpatches, nXpatches, plotTitle=\"Terrain1 patches\")\r\n",
    "\r\n",
    "patches_2 = create_img_patches(terrain2Resized, ySteps, xSteps)\r\n",
    "plotTerrainPatches(patches_2, nYpatches, nXpatches, plotTitle=\"Terrain2 patches\")\r\n",
    "\r\n",
    "# test\r\n",
    "#img_reconstructed = patches_to_img(patches, ySteps, xSteps, nYpatches, nXpatches, plotImage=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Choosing of terrain patch and data creation\r\n",
    "We look at the terrain data patches and choose which to create a fit for"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img1 = patches_1[1]\r\n",
    "img2 = patches_2[3]\r\n",
    "x1, y1, z1 = createTerrainData(img1)\r\n",
    "x2, y2, z2 = createTerrainData(img2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 2D plot of the terrain patches\r\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\r\n",
    "ax1.title.set_text(f\"Terrain patch from terrain1\\nMean:\\\r\n",
    "{np.round(np.mean(img1),decimals=1)}\\nVariance: {np.round(np.var(img1),decimals=1)}\")\r\n",
    "ax1.set_xlabel(\"X\"); ax1.set_ylabel(\"Y\")\r\n",
    "surf1 = ax1.imshow(img1, cmap='gray')\r\n",
    "\r\n",
    "ax2.title.set_text(F\"Terrain patch from terrain2\\nMean:\\\r\n",
    "{np.round(np.mean(img2),decimals=1)}\\nVariance: {np.round(np.var(img2),decimals=1)}\")\r\n",
    "ax2.set_xlabel(\"X\"); ax2.set_ylabel(\"Y\")\r\n",
    "surf2 = ax2.imshow(img2, cmap='gray')\r\n",
    "plt.show()\r\n",
    "\r\n",
    "# 3D plot of the terrain patches\r\n",
    "fig = plt.figure()\r\n",
    "ax1 = fig.add_subplot(1,2,1, projection='3d')\r\n",
    "ax1.title.set_text(f\"3D plot of terrain1 patch\")\r\n",
    "ax1.set_xlabel(\"x\"); ax1.set_ylabel(\"y\"); ax1.set_zlabel(\"z\")\r\n",
    "#ax1.view_init(elev=60., azim=-120.0-70)\r\n",
    "#ax1.view_init(elev=-60., azim=-120.0+30)\r\n",
    "ax1.view_init(elev=-75., azim=-91)\r\n",
    "\r\n",
    "ax1.plot_surface(x1, y1, z1, cmap=cm.coolwarm, linewidth = 0, antialiased=False)\r\n",
    "ax2 = fig.add_subplot(1,2,2, projection='3d')\r\n",
    "ax2.title.set_text(\"3D plot of terrain2 patch\")\r\n",
    "ax2.set_xlabel(\"x\"); ax2.set_ylabel(\"y\"); ax2.set_zlabel(\"z\")\r\n",
    "#ax2.view_init(elev=60., azim=-120.0)\r\n",
    "ax2.view_init(elev=-45., azim=-85.0)\r\n",
    "ax2.plot_surface(x2, y2, z2, cmap=cm.coolwarm, linewidth = 0, antialiased=False)\r\n",
    "plt.show()#-91 -75\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Base input data for least square regression\r\n",
    "We construct the data for least square regression based on preprocessed data. We also set up variables that will be used throughout the exercise.<br>\r\n",
    "Terrain patch from terrain 1 is used as input for our models and our tests "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x, y, z = x1, y1, z1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. OLS on data (Exercise1)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running OLS fit on the data as done in EX1 \r\n",
    "Note that we exlude the calculation of CL for betas, since it is emedded within the model itself. See common.py for that code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "degrees = 8\r\n",
    "z_flat = z.ravel(); z_flat = z_flat.reshape(-1,1)\r\n",
    "z_train_OLS = pd.DataFrame()\r\n",
    "z_hat_train_OLS = pd.DataFrame()\r\n",
    "z_test_OLS = pd.DataFrame()\r\n",
    "z_hat_test_OLS = pd.DataFrame()\r\n",
    "\r\n",
    "OLSrun = []\r\n",
    "for degree in range(1, degrees+1):\r\n",
    "    print(f\"Running OLS fitting on degree{degree}\")\r\n",
    "    X = create_X(x, y, degree) # Design Matrix\r\n",
    "    \r\n",
    "    # Scaling data and splitting it into training and test sets\r\n",
    "    #X_train, X_test, z_train, z_test = prepare_data(X, z, test_size=0.2, shuffle=True, scale_X=False, scale_t=False)\r\n",
    "    X_train, X_test, z_train, z_test = prepare_data(X, z_flat, test_size=0.2, shuffle=True, scale_X=True, scale_t=True, random_state=4155)\r\n",
    "    \r\n",
    "    # Model construction, fitting, and predictions\r\n",
    "    model = OLS(degree=degree) # The model\r\n",
    "    z_hat_train = model.fit(X_train, z_train, SVDfit=False) # Fitting the model and predict on training data\r\n",
    "    z_hat_test = model.predict(X_test) # predict on test data\r\n",
    "    \r\n",
    "    # Evaluatation metrics\r\n",
    "    MSE_score_train = MSE(z_train, z_hat_train)\r\n",
    "    R2_score_train = R2(z_train, z_hat_train)\r\n",
    "    MSE_score_test = MSE(z_test, z_hat_test)\r\n",
    "    R2_score_test = R2(z_test, z_hat_test)\r\n",
    "            \r\n",
    "    # Filling up dataframes for train and test evaluation\r\n",
    "    z_train_OLS[degree] = z_train.flatten() \r\n",
    "    z_hat_train_OLS[degree] = z_hat_train.flatten()\r\n",
    "    z_test_OLS[degree] = z_test.flatten()\r\n",
    "    z_hat_test_OLS[degree] = z_hat_test.flatten()\r\n",
    "\r\n",
    "    # Storing data for all degrees\r\n",
    "    results = {\"X_train\":X_train, \"X_test\":X_test,\"z_train\":z_train, \"z_test\":z_test,\r\n",
    "               \"z_hat_train\":z_hat_train, \"z_hat_test\":z_hat_test, \"model\":model, \"summary\":model.summary()}\r\n",
    "    OLSrun.append(results)\r\n",
    "\r\n",
    "\r\n",
    "# MSE calculations for all degrees\r\n",
    "mse_scores_train = ((z_train_OLS - z_hat_train_OLS) ** 2).mean()\r\n",
    "mse_scores_test = ((z_test_OLS - z_hat_test_OLS) ** 2).mean()\r\n",
    "# R2 calculations for all degrees\r\n",
    "R2_scores_train = 1 - ((z_train_OLS - z_hat_train_OLS) ** 2).sum() / ((z_train_OLS - z_train_OLS.mean())**2).sum() \r\n",
    "R2_scores_test = 1 - ((z_test_OLS - z_hat_test_OLS) ** 2).sum() / ((z_test_OLS - z_test_OLS.mean())**2).sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting performance of OLS for different degrees"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(np.arange(1,degrees+1), mse_scores_test,\"m\", label='MSE on test')\r\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_train,\"c\", label='MSE on train')\r\n",
    "#plt.plot(np.arange(1,degrees+1), R2_scores_test, label='R2 on test')\r\n",
    "#plt.plot(np.arange(1,degrees+1), R2_scores_train, label='R2 on train')\r\n",
    "plt.xlabel(\"Model complexity / Polynomial Degree\")\r\n",
    "plt.ylabel(\"Prediction Error - MSE\")\r\n",
    "\r\n",
    "plt.grid(True)\r\n",
    "plt.legend()\r\n",
    "#plt.savefig(f\"{REPORT_FIGURES}franke_function_OLS_evaluate_fit.pdf\")\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Looking at $\\beta$ values from degree 4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimal_degree = 4\r\n",
    "OLSrunOptimal = OLSrun[optimal_degree-1]\r\n",
    "OLS_summary = OLSrunOptimal[\"summary\"]\r\n",
    "display(OLS_summary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting the fitted terrain image using the most optimal degree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = create_X(x1, y1, 50) # Design Matrix\r\n",
    "X_scaled = standard_scaling(X)\r\n",
    "z1_scaled, scaler = standard_scaling(z1.ravel().reshape(-1,1))\r\n",
    "\r\n",
    "# Model construction, fitting, and predictions\r\n",
    "model = OLS(degree=optimal_degree) # The model\r\n",
    "z_hat_train = model.fit(X_scaled, z1_scaled, SVDfit=False, keep_intercept=True) # Fitting the model and predict on training data\r\n",
    "\r\n",
    "z_hat = scaler.inverse_transform(z_hat_train)\r\n",
    "z_hat = z_hat.reshape((ySteps,xSteps))\r\n",
    "\r\n",
    "# 2D plot of the terrain patches\r\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\r\n",
    "ax1.title.set_text(F\"Terrain to predict\\nMean:\\\r\n",
    "{np.round(np.mean(img1),decimals=1)}\\nVariance: {np.round(np.var(img1),decimals=1)}\")\r\n",
    "ax1.set_xlabel(\"X\"); ax1.set_ylabel(\"Y\")\r\n",
    "surf1 = ax1.imshow(img1, cmap='gray')\r\n",
    "ax2.title.set_text(F\"Predicted terrain using OLS\\nMean:\\\r\n",
    "{np.round(np.mean(z_hat),decimals=1)}\\nVariance: {np.round(np.var(z_hat),decimals=1)}\")\r\n",
    "ax2.set_xlabel(\"X\"); ax2.set_ylabel(\"Y\")\r\n",
    "surf2 = ax2.imshow(z_hat, cmap='gray')\r\n",
    "plt.show()\r\n",
    "\r\n",
    "# 3D plot of predicted terrain patches\r\n",
    "fig = plt.figure()\r\n",
    "ax1 = fig.add_subplot(1,2,1, projection='3d')\r\n",
    "ax1.title.set_text(\"3D plot of Terrain to predict\")\r\n",
    "ax1.set_xlabel(\"x\"); ax1.set_ylabel(\"y\"); ax1.set_zlabel(\"z\")\r\n",
    "ax1.view_init(elev=60., azim=-120.0-70)\r\n",
    "ax1.plot_surface(x1, y1, z1, cmap=cm.coolwarm, linewidth = 0, antialiased=False)\r\n",
    "ax2 = fig.add_subplot(1,2,2, projection='3d')\r\n",
    "ax2.title.set_text(\"3D plot of predicted terrain using OLS\")\r\n",
    "ax2.set_xlabel(\"x\"); ax2.set_ylabel(\"y\"); ax2.set_zlabel(\"z\")\r\n",
    "ax2.view_init(elev=60., azim=-120.0-70)\r\n",
    "ax2.plot_surface(x1, y1, z_hat, cmap=cm.coolwarm, linewidth = 0, antialiased=False)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comments on the OLS fit to terrain data:\r\n",
    "We scale the data since x, y, z is they are not between 0 to 1. Degree of 4 seems to yeild the best performance when fitting to the choosen terrain data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trying to predict all patches using degree 4\r\n",
    "Using patch 1 as a reference, we try to predict all other patches using the same degree. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "testDegree = 150\r\n",
    "testDegree = 4\r\n",
    "patches_1_preds = []\r\n",
    "X = create_X(x1, y1, testDegree) # Design Matrix\r\n",
    "X_scaled = standard_scaling(X)\r\n",
    "\r\n",
    "for patch in tqdm(patches_1):\r\n",
    "    z_scaled = standard_scaling(patch.ravel().reshape(-1,1))\r\n",
    "    model = OLS(degree=testDegree) # The model\r\n",
    "    z_hat_train = model.fit(X_scaled, z_scaled, SVDfit=False, keep_intercept=True) # Fitting the model and predict on training data\r\n",
    "    z_hat = z_hat_train.reshape((ySteps,xSteps))\r\n",
    "    patches_1_preds.append(z_hat)\r\n",
    "    \r\n",
    "terrain1_predicted = patches_to_img(patches_1_preds, ySteps, xSteps, nYpatches, nXpatches, plotImage=False)\r\n",
    "\r\n",
    "# Plotting predicted patches\r\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\r\n",
    "ax1.title.set_text(\"Terrain1 (resized)\")\r\n",
    "ax1.set_xlabel(\"X\"); ax1.set_ylabel(\"Y\")\r\n",
    "surf1 = ax1.imshow(terrain1Resized, cmap='gray')\r\n",
    "ax2.title.set_text(f\"Terrain1 - OLS predicted\\nDegrees used:{testDegree}\")\r\n",
    "ax2.set_xlabel(\"X\"); ax2.set_ylabel(\"Y\")\r\n",
    "surf2 = ax2.imshow(terrain1_predicted, cmap='gray')\r\n",
    "plt.savefig(f\"{REPORT_FIGURES}terrain_OLSpredicted_degree{testDegree}.pdf\")\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A degree of 4 or 5 seems to give a smooth surface for all the predicted patches. We find that the distortion and noise increase in the predicted image when the degree increases above 4-5 considering all patches. At higher degrees, some artifacts within the predicted patches also appear. In the predicted image with all patches, one can see some of the contours of the topographic structures in the image we try to approximate. However, the predicted image that is reconstructed from all the predicted patches is not very accurate. The task of this kind of problem is too complex for an OLS to manage. It may be that having smaller patches would increase the accuracy in reproducing the details incorporated in the input image. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Bias-variance trade-off and resampling techniques on terrain data (Exercise2)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 Setting up variables and data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.random.seed(4155)\r\n",
    "maxdegree = 10\r\n",
    "n_bootstraps = 10\r\n",
    "MSE_test = np.zeros(maxdegree)\r\n",
    "MSE_train = np.zeros(maxdegree)\r\n",
    "polydegree = np.zeros(maxdegree)\r\n",
    "bias = np.zeros(maxdegree)\r\n",
    "variance = np.zeros(maxdegree)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 Testing out different degrees"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for degree in tqdm(range(maxdegree), desc = f\"Looping through polynomials up to {n} degrees with {n_bootstraps} bootstraps: \"):\r\n",
    "    #model = LinearRegression()\r\n",
    "    #model= make_pipeline(PolynomialFeatures(degree=degree), LinearRegression(fit_intercept=False)) \r\n",
    "    X = create_X(x, y, n=degree)\r\n",
    "    #print(f\"X.shape:{X.shape}\")    \r\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, z.reshape(-1,1), test_size=0.2)\r\n",
    "\r\n",
    "    #print(z.shape)\r\n",
    "    #reshape for broadcasting in MSE_test and MSE_val  \r\n",
    "    y_test_ = np.reshape(y_test, newshape=(y_test.shape[0],1))\r\n",
    "    #reshape for broadcasting in MSE_test and MSE_val  \r\n",
    "    y_train_ = np.reshape(y_train, newshape=(y_train.shape[0],1))\r\n",
    "\r\n",
    "    #y_train = np.reshape(y_train, newshape=(y_train.shape[0],1))\r\n",
    "    #TODO: why scale?!?\r\n",
    "\r\n",
    "    scaler = StandardScaler()\r\n",
    "    scaler.fit(X_train)\r\n",
    "    X_train_scaled = scaler.transform(X_train)\r\n",
    "    X_test_scaled = scaler.transform(X_test)\r\n",
    "    y_pred = np.empty((y_test.shape[0], n_bootstraps))\r\n",
    "    y_fit = np.empty((y_train.shape[0], n_bootstraps))\r\n",
    "    \r\n",
    "    for i in range(n_bootstraps):\r\n",
    "        #bootstrap:\r\n",
    "        x_, y_ = resample(X_train_scaled, y_train)\r\n",
    "        #fit model to x_,y_ sample:\r\n",
    "        clf =  LinearRegression().fit(x_, y_)\r\n",
    "        #fit model and predict on test data:\r\n",
    "        y_pred[:, i] = clf.predict(X_test_scaled).ravel()\r\n",
    "        #predict on train data:\r\n",
    "        y_fit[:,i] = clf.predict(x_).ravel()\r\n",
    "        \r\n",
    "\r\n",
    "    polydegree[degree] = degree\r\n",
    "    #print(f\"y_test.shape:{y_test.shape}, y_pred.shape{y_pred.shape}\")\r\n",
    "    MSE_test[degree] = np.mean( np.mean((y_test - y_pred)**2, axis=1, keepdims=True ))\r\n",
    "    MSE_train[degree] = np.mean( np.mean((y_train - y_fit)**2, axis=1, keepdims=True ))\r\n",
    "    bias[degree] = np.mean( (y_test - np.mean(y_pred, axis=1, keepdims=True))**2 )\r\n",
    "    variance[degree] = np.mean( np.var(y_pred, axis=1, keepdims=True))\r\n",
    "    \r\n",
    "    \r\n",
    "plt.plot(polydegree, MSE_test,\"m\", label='MSE_test')\r\n",
    "plt.plot(polydegree, MSE_train,\"c\", label='MSE_train')\r\n",
    "\r\n",
    "plt.plot(polydegree, bias,\"b--\", label='bias')\r\n",
    "plt.plot(polydegree, variance,\"r--\", label='Variance')\r\n",
    "#plt.plot(polydegree, bias+variance,\"g--\", label='bias+variance')\r\n",
    "\r\n",
    "plt.xlabel(\"Model complexity / Polynomial Degree\")\r\n",
    "plt.ylabel(\"Prediction Error\")\r\n",
    "\r\n",
    "plt.grid(True)\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 Studying bias-variance tradeoff as dependance on the number of datpoints"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_list = [10, 20, 30, 40, 50]\r\n",
    "n_bootstraps = 20\r\n",
    "maxdegree = 12\r\n",
    "polydegree = np.arange(maxdegree)\r\n",
    "\r\n",
    "for n in n_list:\r\n",
    "    x = np.sort(np.random.uniform(0,1,n))\r\n",
    "    y = np.sort(np.random.uniform(0,1,n))\r\n",
    "    x,y = np.meshgrid(x,y)\r\n",
    "    z = common.FrankeFunction(x, y) + 0.2*np.random.normal(0, size = n)\r\n",
    "\r\n",
    "    MSE_test_n = np.zeros(maxdegree)\r\n",
    "    MSE_train_n = np.zeros(maxdegree)\r\n",
    "    variance_n = np.zeros(maxdegree)\r\n",
    "    bias_n = np.zeros(maxdegree)\r\n",
    "\r\n",
    "    for degree in tqdm(range(maxdegree), desc = f\"Looping through polynomials up to {maxdegree} degrees with {n_bootstraps} bootstraps: \"):\r\n",
    "        model = common.LinearRegression()\r\n",
    "        #model= make_pipeline(PolynomialFeatures(degree=degree), LinearRegression(fit_intercept=False)) \r\n",
    "        X = common.create_X(x, y, n=degree)    \r\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, z.reshape(-1,1), test_size=0.2)\r\n",
    "\r\n",
    "        #reshape for broadcasting in MSE_test and MSE_val  \r\n",
    "        y_test_ = np.reshape(y_test, newshape=(y_test.shape[0],1))\r\n",
    "        #reshape for broadcasting in MSE_test and MSE_val  \r\n",
    "        y_train_ = np.reshape(y_train, newshape=(y_train.shape[0],1))\r\n",
    "\r\n",
    "        #y_train = np.reshape(y_train, newshape=(y_train.shape[0],1))\r\n",
    "        #TODO: why scale?!?\r\n",
    "\r\n",
    "        scaler = StandardScaler()\r\n",
    "        scaler.fit(X_train)\r\n",
    "        X_train_scaled = scaler.transform(X_train)\r\n",
    "        X_test_scaled = scaler.transform(X_test)\r\n",
    "        y_pred = np.empty((y_test.shape[0], n_bootstraps))\r\n",
    "        y_fit = np.empty((y_train.shape[0], n_bootstraps))\r\n",
    "    \r\n",
    "        for i in range(n_bootstraps):\r\n",
    "            #bootstrap:\r\n",
    "            x_, y_ = resample(X_train_scaled, y_train)\r\n",
    "            #fit model to x_,y_ sample:\r\n",
    "            #print(f\"y_.shape : {y_.shape}\")\r\n",
    "            model.fit(x_, y_, SVDfit=False)\r\n",
    "            #fit model and predict on test data:\r\n",
    "            y_pred[:, i] = model.predict(X_test_scaled).ravel()\r\n",
    "            #predict on train data:\r\n",
    "            y_fit[:,i] = model.predict(X_train_scaled).ravel()\r\n",
    "        \r\n",
    "\r\n",
    "        #polydegree[degree] = degree\r\n",
    "   \r\n",
    "        MSE_test_n[degree] = np.mean( np.mean((y_test_ - y_pred)**2, axis=1, keepdims=True) )\r\n",
    "        MSE_train_n[degree] = np.mean( np.mean((y_train_ - y_fit)**2, axis=1, keepdims=True) )\r\n",
    "        bias_n[degree] = np.mean( (y_test - np.mean(y_pred, axis=1, keepdims=True))**2 )\r\n",
    "        variance_n[degree] = np.mean( np.var(y_pred, axis=1, keepdims=True))\r\n",
    "    \r\n",
    "    plt.plot(polydegree, MSE_test_n,\"m\", label='MSE_test')\r\n",
    "    plt.plot(polydegree, MSE_train_n,\"c\", label='MSE_train')\r\n",
    "\r\n",
    "    plt.plot(polydegree, bias_n,\"g--\", label='bias')\r\n",
    "    plt.plot(polydegree, variance_n,\"r--\", label='Variance')\r\n",
    "\r\n",
    "    plt.title(f\"Bias-Variance tradeoff for n={n} datapoints\")\r\n",
    "    plt.xlabel(\"Model complexity / Polynomial Degree\")\r\n",
    "    plt.ylabel(\"Prediction Error\")\r\n",
    "\r\n",
    "    plt.grid(True)\r\n",
    "    plt.legend()\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tests of dimensions image reduction and patches"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "i = 1\r\n",
    "value = 720\r\n",
    "while True:\r\n",
    "    i += 1\r\n",
    "    if((value % i)==0):\r\n",
    "        print(f\"value:{value / i} at i:{i}\")\r\n",
    "    if(i>=value):\r\n",
    "        break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "i = 1\r\n",
    "value = 360\r\n",
    "while True:\r\n",
    "    i += 1\r\n",
    "    if((value % i)==0):\r\n",
    "        print(f\"value:{value / i} at i:{i}\")\r\n",
    "    if(i>=value):\r\n",
    "        break"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('pytorch_lightning': conda)"
  },
  "interpreter": {
   "hash": "cdd16a4e159c1067d9dfa51729b478ccc85a4bb59359633d71fef71fb1a46b1f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}