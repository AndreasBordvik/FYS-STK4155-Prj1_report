{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 6: Analysis of real data  (score 30 points)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from imageio import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "import os\n",
    "from common import *\n",
    "import cv2\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import linear_model as sk\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "#from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "print(f\"Root directory: {os.getcwd()}\")\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "    \"font.size\": 10,\n",
    "})\n",
    "\n",
    "#%matplotlib"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Global variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "SEED_VALUE = 4155\n",
    "np.random.seed(SEED_VALUE) # Random seed to guarantee reproducibility\n",
    "# Paths\n",
    "#INPUT_DATA = \"data/input_data/\"  # Path for input data\n",
    "#REPORT_DATA = \"data/report_data/\" # Path for data ment for the report\n",
    "#REPORT_FIGURES = \"figures/\" # Path for figures ment for the report\n",
    "# Setting for range of degrees\n",
    "#from_degree = 0\n",
    "#to_degree = 14\n",
    "#degrees = np.arange(from_degree,to_degree)\n",
    "#degree = 5\n",
    "# Setting for logspace range of lambdas \n",
    "#from_lambda = -7 #\n",
    "#to_lambda = 3 #\n",
    "#nLambdas = 10\n",
    "#lambdas = np.logspace(from_lambda, to_lambda, nLambdas)\n",
    "# Rescale settings\n",
    "rescale_factor = 0.2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading and plotting terrain data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load the terrain\n",
    "terrain1_file = \"SRTM_data_Norway_1.tif\"\n",
    "terrain2_file = \"SRTM_data_Norway_2.tif\"\n",
    "terrain1 =  imread(f'{INPUT_DATA}{terrain1_file}')\n",
    "terrain2 = imread(f'{INPUT_DATA}{terrain2_file}')\n",
    "\n",
    "# Plotting terrain\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.title.set_text(\"Terrain over Norway 1\")\n",
    "ax1.set_xlabel(\"X\"); ax1.set_ylabel(\"Y\")\n",
    "surf1 = ax1.imshow(terrain1, cmap='gray')\n",
    "ax2.title.set_text(\"Terrain over Norway 2\")\n",
    "ax2.set_xlabel(\"X\"); ax2.set_ylabel(\"Y\")\n",
    "surf2 = ax2.imshow(terrain2, cmap='gray')\n",
    "plt.savefig(f\"{REPORT_FIGURES}{EX6}terrain_data.pdf\")\n",
    "plt.show()\n",
    "print(terrain1.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Thoughts on the topographical data\n",
    "Before we proceed with this exercise, we want to briefly discuss the nature of this terrain data compared to the generated frank function. We regard the franke function as generic because it is known and its behavior or shape can be generated for other values of x and y. Thus, creating a model that can generalize the function even with added noise for other unknown data points for x and y makes sense. However, we are uncertain if this idea of generalization is transferable to topographical terrain data in the same sense. The terrain data given is unique, and its shape cannot be generalized for unknown data points. If the purpose is to create the absolute best fit for the specific and unique terrain data, one could simply overfit to the terrain data by having an extremely high degree when fitting a model, and that is not really ML. One could probably argue that a model should tackle added noise on the terrain data and still be able to represent the shapes and contours incorporated in the image. However, images are what they are, and in this case, the topographical terrain \"is what it is.\"  With that being said, we proceed with this exercise in the same spirit as done for exercises 1-5 even though we question this use case when working on this kind of real data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3D plot of the whole Terrain image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make data for terrain1\n",
    "z1 = np.array(terrain1)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(z1)\n",
    "z1_scaled = scaler.transform(z1)\n",
    "#y1 = np.linspace(0,1,z1.shape[0])\n",
    "#x1 = np.linspace(0,1,z1.shape[1])\n",
    "y1 = np.arange(0, z1.shape[0])\n",
    "x1 = np.arange(0, z1.shape[1])\n",
    "x1_m, y1_m = np.meshgrid(x1,y1)\n",
    "\n",
    "# Make data for terrain2\n",
    "z2 = np.array(terrain2)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(z2)\n",
    "z2_scaled = scaler.transform(z2)\n",
    "#y2 = np.linspace(0,1,z2.shape[0])\n",
    "#x2 = np.linspace(0,1,z2.shape[1])\n",
    "y2 = np.arange(0, z2.shape[0])\n",
    "x2 = np.arange(0, z2.shape[1])\n",
    "x2_m, y2_m = np.meshgrid(x2,y2)\n",
    "\n",
    "#%matplotlib\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,2,1, projection='3d')\n",
    "ax1.title.set_text(\"Terrain1 plot\")\n",
    "ax1.set_xlabel(\"x\"); ax1.set_ylabel(\"y\"); ax1.set_zlabel(\"z\")\n",
    "ax1.plot_surface(x1_m, y1_m, z1, cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2, projection='3d')\n",
    "ax2.title.set_text(\"Terrain2 plot\")\n",
    "ax2.set_xlabel(\"x\"); ax2.set_ylabel(\"y\"); ax2.set_zlabel(\"z\")\n",
    "ax2.plot_surface(x2_m, y2_m, z2, cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.0. Terrain data - Preprocessing and transformation\r\n",
    "Least Square regression is not designed to tackle images directly. Thus, we must first transform the terrain data by slicing it into several bits and pieces. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.0.1 Resizeing the terrain image\r\n",
    "For computational purpose, we resize the terrain image to have a resonable amount of datapoints for our least sqaure models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ySize = int(terrain1.shape[0] * rescale_factor); print(ySize)\n",
    "xSize = int(terrain1.shape[1] * rescale_factor); print(xSize)\n",
    "terrain1Resized = cv2.resize(terrain1, (xSize, ySize))\n",
    "terrain2Resized = cv2.resize(terrain2, (xSize, ySize))\n",
    "\n",
    "# Plotting terrain\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.title.set_text(\"Terrain over Norway 1 (Resized)\")\n",
    "ax1.set_xlabel(\"X\"); ax1.set_ylabel(\"Y\")\n",
    "surf1 = ax1.imshow(terrain1Resized, cmap='gray')\n",
    "ax2.title.set_text(\"Terrain over Norway 2 (Resized)\")\n",
    "ax2.set_xlabel(\"X\"); ax2.set_ylabel(\"Y\")\n",
    "surf2 = ax2.imshow(terrain2Resized, cmap='gray')\n",
    "plt.savefig(f\"{REPORT_FIGURES}{EX6}terrain_data_resized.pdf\")\n",
    "plt.show()\n",
    "print(terrain1[0,0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.0.2 Creating image patches and Terrain data selection"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Methods"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_img_patches(img, ySteps, xSteps):\n",
    "    patches = []\n",
    "    for y in range(0,img.shape[0], ySteps):\n",
    "        for x in range(0,img.shape[1], xSteps):\n",
    "            y_from = y; \n",
    "            y_to = y+ySteps; \n",
    "            x_from = x; \n",
    "            x_to = x+xSteps; \n",
    "            img_patch = img[y_from:y_to, x_from:x_to]        \n",
    "            patches.append(img_patch)\n",
    "\n",
    "    return patches\n",
    "\n",
    "def patches_to_img(patches, ySteps, xSteps, nYpatches, nXpatches, plotImage=False):\n",
    "    img = np.zeros((ySteps*nYpatches, xSteps*nXpatches))\n",
    "    i = 0\n",
    "    for y in range(0,img.shape[0], ySteps):\n",
    "        for x in range(0,img.shape[1], xSteps):\n",
    "            y_from = y; \n",
    "            y_to = y+ySteps; \n",
    "            x_from = x; \n",
    "            x_to = x+xSteps; \n",
    "            img[y_from:y_to, x_from:x_to] = patches[i]         \n",
    "            i += 1\n",
    "    \n",
    "    if plotImage:\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(\"Reconstructed img\")\n",
    "        plt.show()\n",
    "    return img\n",
    "\n",
    "def plotTerrainPatches(patches, nYpatches, nXpatches, plotTitle=\"Terrain patches\"):\n",
    "    # Plotting terrain patches\n",
    "    fig, ax = plt.subplots(nYpatches, nXpatches,figsize=(4,8))\n",
    "    i=0\n",
    "    for y in range(nYpatches):\n",
    "        for x in range(nXpatches):\n",
    "            ax[y,x].title.set_text(f\"Patch{i}\")\n",
    "            ax[y,x].set_xlabel(\"X\"); ax1.set_ylabel(\"Y\")\n",
    "            ax[y,x].imshow(patches[i], cmap='gray')\n",
    "            i+=1\n",
    "    \n",
    "    fig.suptitle(f\"{plotTitle}\") # or plt.suptitle('Main title')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{REPORT_FIGURES}{EX6}{plotTitle}.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "def createTerrainData(terrain, includeMeshgrid=True):\n",
    "    z = np.array(terrain) \n",
    "    x = np.arange(0, z.shape[1])\n",
    "    y = np.arange(0, z.shape[0])\n",
    "    if includeMeshgrid:\n",
    "        x, y = np.meshgrid(x,y)\n",
    "    return x,y,z\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nXpatches = 2; nYpatches=4\n",
    "ySteps = int(terrain2Resized.shape[0] / nYpatches)\n",
    "xSteps = int(terrain2Resized.shape[1] / nXpatches)\n",
    "\n",
    "patches_1 = create_img_patches(terrain1Resized, ySteps, xSteps)\n",
    "plotTerrainPatches(patches_1, nYpatches, nXpatches, plotTitle=\"Terrain1 patches\")\n",
    "\n",
    "patches_2 = create_img_patches(terrain2Resized, ySteps, xSteps)\n",
    "plotTerrainPatches(patches_2, nYpatches, nXpatches, plotTitle=\"Terrain2 patches\")\n",
    "\n",
    "# test\n",
    "#img_reconstructed = patches_to_img(patches, ySteps, xSteps, nYpatches, nXpatches, plotImage=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.0.3 Choosing of terrain patch and data creation\r\n",
    "We look at the terrain data patches and choose which to create a fit for"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img1 = patches_1[1]\n",
    "img2 = patches_2[3]\n",
    "x1, y1, z1 = createTerrainData(img1)\n",
    "x2, y2, z2 = createTerrainData(img2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scaler1 = MinMaxScaler()\n",
    "scaler1.fit(img1)\n",
    "img1_normalized = scaler1.transform(img1)\n",
    "norm_var1 = np.round(np.var(img1_normalized),decimals=4)\n",
    "\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler2.fit(img2)\n",
    "img2_normalized = scaler1.transform(img2)\n",
    "norm_var2 = np.round(np.var(img2_normalized),decimals=4)\n",
    "\n",
    "# 2D plot of the terrain patches\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.title.set_text(f\"Terrain patch from terrain1\\nMean:\\\n",
    "{np.round(np.mean(img1),decimals=1)}\\nVariance: {np.round(np.var(img1),decimals=1)}\\n\\\n",
    "normalized variance: {norm_var1}\")\n",
    "ax1.set_xlabel(\"X\"); ax1.set_ylabel(\"Y\")\n",
    "surf1 = ax1.imshow(img1, cmap='gray')\n",
    "\n",
    "ax2.title.set_text(F\"Terrain patch from terrain2\\nMean:\\\n",
    "{np.round(np.mean(img2),decimals=1)}\\nVariance: {np.round(np.var(img2),decimals=1)}\\n\\\n",
    "normalized variance: {norm_var2}\")\n",
    "ax2.set_xlabel(\"X\"); ax2.set_ylabel(\"Y\")\n",
    "surf2 = ax2.imshow(img2, cmap='gray')\n",
    "plt.savefig(f\"{REPORT_FIGURES}{EX6}terrain_patch_to_fit_2D.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# 3D plot of the terrain patches\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,2,1, projection='3d')\n",
    "ax1.title.set_text(f\"3D plot of terrain1 patch\")\n",
    "ax1.set_xlabel(\"x\"); ax1.set_ylabel(\"y\"); ax1.set_zlabel(\"z\")\n",
    "#ax1.view_init(elev=60., azim=-120.0-70)\n",
    "#ax1.view_init(elev=-60., azim=-120.0+30)\n",
    "ax1.view_init(elev=-75., azim=-91)\n",
    "ax1.plot_surface(x1, y1, z1, cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2, projection='3d')\n",
    "ax2.title.set_text(\"3D plot of terrain2 patch\")\n",
    "ax2.set_xlabel(\"x\"); ax2.set_ylabel(\"y\"); ax2.set_zlabel(\"z\")\n",
    "#ax2.view_init(elev=60., azim=-120.0)\n",
    "ax2.view_init(elev=-45., azim=-85.0)\n",
    "ax2.plot_surface(x2, y2, z2, cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n",
    "plt.savefig(f\"{REPORT_FIGURES}{EX6}terrain_patch_to_fit_3D.pdf\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.0.4 Base input data for least square regression\r\n",
    "We construct the data for least square regression based on preprocessed data. We also set up variables that will be used throughout the exercise.<br>\r\n",
    "Terrain patch from terrain 1 is used as input for our models and our tests "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "terrain_data = 1\n",
    "\n",
    "if terrain_data == 1: # Choosing terrain1\n",
    "    x, y, z = x1, y1, z1.copy() \n",
    "    #z_min = np.min(z)\n",
    "    z_max = np.max(z)\n",
    "    z = z1\n",
    "\n",
    "elif terrain_data == 2: # Choosing terrain2\n",
    "    x, y, z = x2, y2, z2.copy() \n",
    "    #z_min = np.min(z)\n",
    "    z_max = np.max(z)\n",
    "    z = z2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.1 OLS on data (Exercise1)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running OLS fit on the data as done in EX1 \r\n",
    "Note that we exlude the calculation of CL for betas, since it is emedded within the model itself. See common.py for that code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "z_flat = z.ravel(); z_flat = z_flat.reshape(-1,1)\n",
    "z_train_OLS = pd.DataFrame()\n",
    "z_hat_train_OLS = pd.DataFrame()\n",
    "z_test_OLS = pd.DataFrame()\n",
    "z_hat_test_OLS = pd.DataFrame()\n",
    "X_test_OLS = {}\n",
    "df = pd.DataFrame()\n",
    "\n",
    "degrees = 24\n",
    "scale_X = True\n",
    "scale_z = True\n",
    "test_size = 0.1\n",
    "for degree in range(1, degrees+1):\n",
    "    print(f\"Running OLS fitting on degree{degree}\")\n",
    "    X = create_X(x, y, degree) # Design Matrix\n",
    "    \n",
    "    # Scaling data and splitting it into training and test sets\n",
    "    if scale_X:\n",
    "        if scale_z:\n",
    "            X_train, X_test, z_train, z_test = prepare_data(X, z_flat, test_size=test_size, shuffle=True, scale_X=True, scale_t=True, random_state=SEED_VALUE)\n",
    "        else:\n",
    "            X_train, X_test, z_train, z_test = prepare_data(X, z_flat, test_size=test_size, shuffle=True, scale_X=True, scale_t=False, random_state=SEED_VALUE)\n",
    "    else:\n",
    "         X_train, X_test, z_train, z_test = prepare_data(X, z_flat, test_size=test_size, shuffle=True, scale_X=False, scale_t=False, random_state=SEED_VALUE)\n",
    "    \n",
    "    # Model construction, fitting, and predictions\n",
    "    model = OLS(degree=degree) # The model\n",
    "    z_hat_train = model.fit(X_train, z_train) # Fitting the model and predict on training data\n",
    "    z_hat_test = model.predict(X_test) # predict on test data\n",
    "    \n",
    "    # Evaluatation metrics\n",
    "    MSE_score_train = MSE(z_train, z_hat_train)\n",
    "    R2_score_train = R2(z_train, z_hat_train)\n",
    "    MSE_score_test = MSE(z_test, z_hat_test)\n",
    "    R2_score_test = R2(z_test, z_hat_test)\n",
    "            \n",
    "    # Filling up dataframes for train and test evaluation\n",
    "    summary_df = model.summary()\n",
    "    df = pd.concat([df,summary_df], axis=0)\n",
    "\n",
    "    z_train_OLS[degree] = z_train.flatten() \n",
    "    z_hat_train_OLS[degree] = z_hat_train.flatten()\n",
    "    z_test_OLS[degree] = z_test.flatten()\n",
    "    z_hat_test_OLS[degree] = z_hat_test.flatten()\n",
    "    X_test_OLS[f\"{degree}\"] = X_test\n",
    "\n",
    "    # Storing data for all degrees\n",
    "    results = {\"X_train\":X_train, \"X_test\":X_test,\"z_train\":z_train, \"z_test\":z_test,\n",
    "               \"z_hat_train\":z_hat_train, \"z_hat_test\":z_hat_test, \"model\":model, \"summary\":summary_df}\n",
    "    #OLSrun.append(results)\n",
    "\n",
    "\n",
    "# MSE calculations for all degrees\n",
    "mse_scores_train = ((z_train_OLS - z_hat_train_OLS) ** 2).mean()\n",
    "mse_scores_test = ((z_test_OLS - z_hat_test_OLS) ** 2).mean()\n",
    "# R2 calculations for all degrees\n",
    "R2_scores_train = 1 - ((z_train_OLS - z_hat_train_OLS) ** 2).sum() / ((z_train_OLS - z_train_OLS.mean())**2).sum() \n",
    "R2_scores_test = 1 - ((z_test_OLS - z_hat_test_OLS) ** 2).sum() / ((z_test_OLS - z_test_OLS.mean())**2).sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting performance of OLS for different degrees"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "degrees = 24\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_test,\"m\", label=f'MSE on test (fraction:{test_size})')\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_train,\"c\", label='MSE on train')\n",
    "#plt.plot(np.arange(1,degrees+1), R2_scores_test, label='R2 on test')\n",
    "#plt.plot(np.arange(1,degrees+1), R2_scores_train, label='R2 on train')\n",
    "plt.xlabel(\"Model complexity / Polynomial Degree\")\n",
    "plt.ylabel(\"Prediction Error - MSE\")\n",
    "plt.xticks(np.arange(1,degrees+1))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(f\"{REPORT_FIGURES}{EX6_1}terrain_patch_OLS_evaluate_fit_test_frac{test_size}.pdf\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Looking at $\\beta$ values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "degree_insight = 8\n",
    "df_degree = df[df[\"degree\"] == degree_insight]\n",
    "print(\"df_degree.shape:\",df_degree.shape)\n",
    "display(df_degree)\n",
    "df_degree.to_csv(f\"{REPORT_DATA}{EX6_1}OLS_beta_error_degree{degree_insight}.csv\")\n",
    "\n",
    "fig = plot_beta_errors(df_degree, degree_insight)\n",
    "fig.savefig(f\"{REPORT_FIGURES}{EX6_1}OLS_beta_error_degree{degree_insight}.pdf\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hva skjer når vi variaere K... ? Hva Størrelse på de forskjellige foldsa å si mellom foldd, og størrelse på dataen. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## We resized the image to get fewer datapoints\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting the fitted terrain image using the optimal degree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "degrees_insight = [4,12,19]\n",
    "z_hats = [z]\n",
    "for deg in degrees_insight:\n",
    "    X = create_X(x, y, deg) # Design Matrix \n",
    "    X_scaled, _ = standard_scaling_single(X)\n",
    "    z_scaled, z_scaler = standard_scaling_single(z.ravel().reshape(-1,1))\n",
    "\n",
    "    # Model construction, fitting, and predictions\n",
    "    model = OLS(degree=deg) # The model\n",
    "    z_hat_train = model.fit(X_scaled, z_scaled, SVDfit=False, keep_intercept=True) # Fitting the model and predict on training data\n",
    "\n",
    "    z_hat = z_scaler.inverse_transform(z_hat_train)\n",
    "    z_hat = z_hat.reshape((ySteps,xSteps))\n",
    "    z_hats.append(z_hat)\n",
    "\n",
    "z_hats = np.array(z_hats)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 2D plot of predicted terrain patches\n",
    "fig = plt.figure(figsize=(7,16))\n",
    "j = 0\n",
    "for i in range(z_hats.shape[0]):\n",
    "    ax = fig.add_subplot(z_hats.shape[0],2,1+i)\n",
    "    title = f\"Predicted terrain\\n(OLS degree {degrees_insight[j]})\" if i>0 else \"Target terrain\"\n",
    "    ax.title.set_text(f\"{title}\\nMean:\\\n",
    "    {np.round(np.mean(z_hats[i]),decimals=1)}\\nVariance: {np.round(np.var(z_hats[i]),decimals=1)}\")\n",
    "    \n",
    "    ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\")\n",
    "    surf2 = ax.imshow(z_hats[i], cmap='gray')\n",
    "    j+=1 if i > 0 else j\n",
    "\n",
    "plt.suptitle(\"2D plot of Terrain\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{REPORT_FIGURES}{EX6_1}target_terrain_and_OLS_prediction_2D.pdf\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 3D plot of predicted terrain patches\n",
    "fig = plt.figure(figsize=(7,16))\n",
    "j = 0\n",
    "for i in range(z_hats.shape[0]):\n",
    "    ax = fig.add_subplot(z_hats.shape[0],2,1+i, projection='3d')\n",
    "    title = f\"Predicted terrain\\n(OLS degree {degrees_insight[j]})\" if i>0 else \"Target terrain\"\n",
    "    ax.title.set_text(title)\n",
    "    ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_zlabel(\"z\")\n",
    "    #ax2.view_init(elev=60., azim=-120.0-70)\n",
    "    ax.view_init(elev=-75., azim=-91)\n",
    "    ax.plot_surface(x, y, z_hats[i], cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n",
    "    j+=1 if i > 0 else j\n",
    "\n",
    "plt.suptitle(\"3D plot of Terrain\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{REPORT_FIGURES}{EX6_1}target_terrain_and_OLS_prediction_3D.pdf\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comments on the OLS fit to terrain data:\r\n",
    "We scale the data since x, y, z is they are not between 0 to 1. Degree of 4 seems to yeild the best performance when fitting to the choosen terrain data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trying to overfit to terrain for all patches"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "testDegree = 150\n",
    "testDegree = 4\n",
    "patches_1_preds = []\n",
    "X = create_X(x1, y1, testDegree) # Design Matrix\n",
    "X_scaled, _ = standard_scaling_single(X)\n",
    "\n",
    "for patch in tqdm(patches_1):\n",
    "    z_scaled, _ = standard_scaling_single(patch.ravel().reshape(-1,1))\n",
    "    model = OLS(degree=testDegree) # The model\n",
    "    z_hat_train = model.fit(X_scaled, z_scaled, SVDfit=False, keep_intercept=True) # Fitting the model and predict on training data\n",
    "    z_hat = z_hat_train.reshape((ySteps,xSteps))\n",
    "    patches_1_preds.append(z_hat)\n",
    "    \n",
    "terrain1_predicted = patches_to_img(patches_1_preds, ySteps, xSteps, nYpatches, nXpatches, plotImage=False)\n",
    "\n",
    "# Plotting predicted patches\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.title.set_text(f\"Terrain1 (resized)\")\n",
    "ax1.set_xlabel(\"X\"); ax1.set_ylabel(\"Y\")\n",
    "surf1 = ax1.imshow(terrain1Resized, cmap='gray')\n",
    "ax2.title.set_text(f\"Terrain1 - OLS predicted\")\n",
    "ax2.set_xlabel(\"X\"); ax2.set_ylabel(\"Y\")\n",
    "surf2 = ax2.imshow(terrain1_predicted, cmap='gray')\n",
    "plt.savefig(f\"{REPORT_FIGURES}{EX6_1}terrain_and_all_patches_predicted_at_degree{testDegree}.pdf\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO:\r\n",
    "#### This text may have to be adjusted\r\n",
    "A degree of 4 or 5 seems to give a smooth surface for all the predicted patches. We find that the distortion and noise increase in the predicted image when the degree increases above 4-5 considering all patches. At higher degrees, some artifacts within the predicted patches also appear. In the predicted image with all patches, one can see some of the contours of the topographic structures in the image we try to approximate. However, the predicted image that is reconstructed from all the predicted patches is not very accurate. The task of this kind of problem is too complex for an OLS to manage. It may be that having smaller patches would increase the accuracy in reproducing the details incorporated in the input image. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.2 Bias-variance trade-off and resampling techniques on terrain data (Exercise2)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2.1 Setting up variables and data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.random.seed(SEED_VALUE)\n",
    "maxdegree = 10\n",
    "n_bootstraps = 10\n",
    "MSE_test = np.zeros(maxdegree)\n",
    "MSE_train = np.zeros(maxdegree)\n",
    "polydegree = np.arange(1,maxdegree+1)\n",
    "bias = np.zeros(maxdegree)\n",
    "variance = np.zeros(maxdegree)\n",
    "z_flat = z.ravel().reshape(-1,1); \n",
    "z_flat = z_flat.reshape(-1,1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2.2 Testing out different degrees while resampling using bootstrap"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for degree in tqdm(range(1, maxdegree+1), desc = f\"Looping through polynomials up to {maxdegree} degrees with {n_bootstraps} bootstraps: \"):\n",
    "    #model= make_pipeline(PolynomialFeatures(degree=degree), LinearRegression(fit_intercept=False)) \n",
    "    X = create_X(x, y, n=degree)\n",
    "\n",
    "    # Train test split\n",
    "    X_train, X_test, z_train, z_test = prepare_data(X, z_flat, test_size=0.2, shuffle=True, scale_X=True, scale_t=True, random_state=SEED_VALUE)\n",
    "\n",
    "    #TODO: why scale?!?\n",
    "    # How do we scale? should test and train have different scalers or use the same?\n",
    "    # Scaling\n",
    "    \n",
    "    model =  OLS(degree=degree)\n",
    "    z_hat_trains, z_hat_tests = bootstrapping(X_train, z_train, X_test, z_test, \n",
    "                                              n_bootstraps, model, keep_intercept=True)\n",
    "      \n",
    "    MSE_test[degree-1] = np.mean( np.mean((z_test - z_hat_tests)**2, axis=1, keepdims=True ))\n",
    "    MSE_train[degree-1] = np.mean( np.mean((z_train - z_hat_trains)**2, axis=1, keepdims=True ))\n",
    "    bias[degree-1] = np.mean( (z_test - np.mean(z_hat_tests, axis=1, keepdims=True))**2 )\n",
    "    variance[degree-1] = np.mean( np.var(z_hat_tests, axis=1, keepdims=True))\n",
    "       \n",
    "plt.plot(polydegree, MSE_test,\"m\", label='MSE test')\n",
    "plt.plot(polydegree, MSE_train,\"c\", label='MSE train')\n",
    "plt.plot(polydegree, bias,\"b--\", label='bias')\n",
    "plt.plot(polydegree, variance,\"r--\", label='Variance')\n",
    "#plt.plot(polydegree, bias+variance,\"g--\", label='bias+variance')\n",
    "\n",
    "plt.xlabel(\"Model complexity / Polynomial Degree\")\n",
    "plt.ylabel(\"Prediction Error - MSE\")\n",
    "plt.xticks(polydegree)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(f\"{REPORT_FIGURES}{EX6_2}model_complexity_using_bootstrap.pdf\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.3 Cross-validation as resampling techniques, adding more complexity (Exercise3)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3.1 Cross validation algorithm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "np.random.seed(SEED_VALUE)\n",
    "def cross_val(X_data:np.ndarray, t_data:np.ndarray, model: object, k_folds_from = 5,k_folds_to = 11)-> Tuple[list, list]:\n",
    "    t_data = t_data.ravel()\n",
    "    t_data = np.expand_dims(t_data, axis=1)\n",
    "    numb_of_its = k_folds_to-k_folds_from\n",
    "    mean_MSE_test = np.zeros(numb_of_its)\n",
    "    mean_MSE_train = np.zeros(numb_of_its)\n",
    "    \n",
    "    inds = np.arange(t_data.shape[0])\n",
    "    #shuffle indicies to create random folds: \n",
    "    np.random.shuffle(inds)\n",
    "    \n",
    "    #create array for plotting:\n",
    "    folds = np.zeros(numb_of_its)\n",
    "    for fold in range(k_folds_from,k_folds_to):\n",
    "        #split inds into multiple sub-arrays of equal or aprox equal size: \n",
    "        fold_inds = np.array_split(inds, fold)\n",
    "        MSE_test = np.zeros(fold)\n",
    "        MSE_train = np.zeros(fold)\n",
    "        folds[fold-k_folds_from] = fold \n",
    "        for k in range(fold):\n",
    "            #get array elements at indx k from folds_inds and use as test data: \n",
    "            X_test = X_data[fold_inds[k],:]\n",
    "            z_test = t_data[fold_inds[k]]\n",
    "    \n",
    "            #create training data by removing test-indicies from data\n",
    "            X_train = np.delete(arr = X_data, obj = fold_inds[k],axis = 0)\n",
    "            z_train = np.delete(t_data, fold_inds[k]) \n",
    "            z_train = np.expand_dims(z_train, axis=1)\n",
    "      \n",
    "            # Scale data: \n",
    "            X_train, _ = standard_scaling(X_train)\n",
    "            z_train, _ = standard_scaling(z_train)\n",
    "            X_test, _ = standard_scaling(X_test)\n",
    "            z_test, _ = standard_scaling(z_test)\n",
    "            \n",
    "            #fit model and predict on train data:\n",
    "            z_hat_train = model.fit(X_train, z_train, SVDfit=False)\n",
    "            \n",
    "            #predict on test data:\n",
    "            z_hat_test = model.predict(X_test)\n",
    "            \n",
    "            # Storing MSE values\n",
    "            MSE_test[k] = np.mean((z_test - z_hat_test)**2)\n",
    "            MSE_train[k] = np.mean((z_train - z_hat_train)**2) \n",
    "            \n",
    "        mean_MSE_test[fold-k_folds_to] = np.mean(MSE_test)\n",
    "        mean_MSE_train[fold-k_folds_to] = np.mean(MSE_train)\n",
    "        \n",
    "    return mean_MSE_test,mean_MSE_train, folds\n",
    "\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3.2 Setting up variables and data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "z_flat = z.ravel().reshape(-1,1); \n",
    "z_flat = z_flat.reshape(-1,1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3.3 Testing out different degrees"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "degree = 10\n",
    "mean_folds_error = np.zeros(6)\n",
    "\n",
    "for folds in range(5,11):\n",
    "    sk_model = sk.LinearRegression()\n",
    "    sk_scores = cross_val_score(sk_model, X, z_flat, cv=folds, scoring = \"neg_mean_squared_error\")\n",
    "    \n",
    "    implemented_scores = cross_val(k = folds, model = \"OLS\", X = X, z = z_flat, shuffle=True)\n",
    "    #plt.plot(np.arange(folds), score,\"-o\", label = f\"{folds} folds\")\n",
    "    plt.plot(np.arange(folds), sk_scores*-1,\"--o\", label = f\"{folds} folds(sk.cv)\")\n",
    "    plt.xlabel(\"fold used as test data\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    mean_folds_error[folds-5] = np.mean(implemented_scores)\n",
    "    #print(sk_scores)\n",
    "    #print(implemented_scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(np.arange(5,11), mean_folds_error)\n",
    "plt.xlabel(\"Number of folds\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "maxdegree = 5\n",
    "for degree in range(1, maxdegree+1):\n",
    "    X = create_X(x, y, n=degree)  \n",
    "    MSE_test, MSE_train, folds = cross_val(X_data= X, t_data=z, model=OLS(degree))\n",
    "    plt.plot(folds, MSE_test,\"m\", label='MSE test')\n",
    "    plt.plot(folds, MSE_train,\"c\", label='MSE train')\n",
    "\n",
    "    plt.xlabel(f\"Folds at polynomial degree:{degree}\")\n",
    "    plt.ylabel(\"Prediction Error - MSE\")\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{REPORT_FIGURES}{EX6_3}model_complexity_using_CV_degree{degree}.pdf\")\n",
    "    plt.show()\n",
    "\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.4 Ridge Regression on the Franke function with resampling (Exercise4)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.4.1 Setting up variables and data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nbf_lambdas = 10\n",
    "lambdas = np.logspace(-5,5, nbf_lambdas)\n",
    "z_train_ridge = pd.DataFrame()\n",
    "z_hat_train_ridge = pd.DataFrame()\n",
    "z_test_ridge = pd.DataFrame()\n",
    "z_hat_test_ridge = pd.DataFrame()\n",
    "z_hat_train_ridge_sk = pd.DataFrame()\n",
    "z_hat_test_ridge_sk = pd.DataFrame()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.4.2 Ridge regression - finding optimal lambda without resampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for lmb in lambdas:\n",
    "    #X_train, X_test, z_train, z_test = prepare_data(X, z, test_size=0.2, shuffle=True, scale_X=False, scale_t=False)\n",
    "    X_train, X_test, z_train, z_test = prepare_data(X, z_flat, test_size=0.2, shuffle=True, \n",
    "                                                    zero_center = False, scale_X=True, scale_t=True, \n",
    "                                                    random_state=SEED_VALUE)\n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"X_train.shape:{X_train.shape}\")\n",
    "    print(f\"z_train.shape:{z_train.shape}\")\n",
    "    print(f\"X_test.shape:{X_test.shape}\")\n",
    "    print(f\"z_test.shape:{z_test.shape}\")\n",
    "    \"\"\"\n",
    "    # Fitting the scikit Ridge model\n",
    "    model_sk = lm.Ridge(lmb, fit_intercept=True)\n",
    "    model_sk.fit(X_train, z_train)\n",
    "    \n",
    "    # Predictions sklearn Ridge\n",
    "    z_hat_train_sk = model_sk.predict(X_train) # predict on train data\n",
    "    z_hat_test_sk = model_sk.predict(X_test) # predict on test data\n",
    "    \n",
    "    # Fitting our Ridge model and predicting on training data\n",
    "    model = RidgeRegression(lmb) # The model\n",
    "    z_hat_train = model.fit(X_train, z_train, SVDfit=False, keep_intercept=False) # Fitting the model    \n",
    "    z_hat_test = model.predict(X_test) # predict on test data\n",
    "\n",
    "    # Filling up sk dataframes\n",
    "    z_hat_train_ridge_sk[lmb] = z_hat_train_sk.flatten()\n",
    "    z_hat_test_ridge_sk[lmb] = z_hat_test_sk.flatten()\n",
    "    \n",
    "    # Filling up dataframes\n",
    "    z_train_ridge[lmb] = z_train.flatten() \n",
    "    z_hat_train_ridge[lmb] = z_hat_train.flatten()\n",
    "    z_test_ridge[lmb] = z_test.flatten()\n",
    "    z_hat_test_ridge[lmb] = z_hat_test.flatten()\n",
    "\n",
    "# MSE calculations for all lambda values\n",
    "mse_scores_train = ((z_train_ridge - z_hat_train_ridge) ** 2).mean()\n",
    "mse_scores_test = ((z_test_ridge - z_hat_test_ridge) ** 2).mean()\n",
    "mse_scors_train_sk = ((z_train_ridge - z_hat_train_ridge_sk) ** 2).mean()\n",
    "mse_scores_test_sk = ((z_test_ridge - z_hat_test_ridge_sk) ** 2).mean()\n",
    "\n",
    "# R2 calculations for all lambda values\n",
    "R2_scores_train = 1 - ((z_train_ridge - z_hat_train_ridge) ** 2).sum() / ((z_train_ridge - z_train_ridge.mean())**2).sum() \n",
    "R2_scores_test = 1 - ((z_test_ridge - z_hat_test_ridge) ** 2).sum() / ((z_test_ridge - z_test_ridge.mean())**2).sum()  \n",
    "\n",
    "# Plots\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.plot(-np.log(lambdas), mse_scores_train, label=\"Training data\")\n",
    "plt.plot(-np.log(lambdas), mse_scores_test, label=\"Test data\")\n",
    "\n",
    "plt.plot(-np.log(lambdas), mse_scors_train_sk, 'm--', label=\"Training data sklearn\")\n",
    "plt.plot(-np.log(lambdas), mse_scores_test_sk, 'y--', label=\"Test data sklearn\")\n",
    "\n",
    "#plt.plot(mse_scores_train,-np.log(lambdas), label=\"Training data\")\n",
    "#plt.plot(mse_scores_test,-np.log(lambdas), label=\"Test data\")\n",
    "plt.xlabel(\"log(lambda)\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Training evaluation on Ridge regression fit\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{REPORT_FIGURES}{EX6_4}Ridge_finding_optimal_lambda_without_resampling.pdf\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.4.3 Ridge regression - finding optimal lambda using bootstrap"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_bootstraps = 100\n",
    "maxdegree = 12\n",
    "\n",
    "#lambdas = [-5,4]\n",
    "i = 0\n",
    "for lmb in lambdas:\n",
    "    MSE_test = np.zeros(maxdegree)\n",
    "    MSE_train = np.zeros(maxdegree)\n",
    "    polydegree = np.arange(maxdegree)\n",
    "    bias = np.zeros(maxdegree)\n",
    "    variance = np.zeros(maxdegree)\n",
    "\n",
    "    for degree in tqdm(range(maxdegree), desc = f\"Looping through polynomials up to {maxdegree} degrees with {n_bootstraps} bootstraps: \"):\n",
    "        #model= make_pipeline(PolynomialFeatures(degree=degree), LinearRegression(fit_intercept=False)) \n",
    "        X = create_X(x, y, n=degree)\n",
    "\n",
    "        # Train test split\n",
    "        X_train, X_test, z_train, z_test = prepare_data(X, z_flat, test_size=0.2, shuffle=True, scale_X=True, scale_t=True, random_state=SEED_VALUE)\n",
    "\n",
    "        #TODO: why scale?!?\n",
    "        # How do we scale? should test and train have different scalers or use the same?\n",
    "        # Scaling\n",
    "\n",
    "        model =  RidgeRegression(lambda_val=lmb)\n",
    "        z_hat_trains, z_hat_tests = bootstrapping(X_train, z_train, X_test, z_test, \n",
    "                                                  n_bootstraps, model, keep_intercept=False)\n",
    "            \n",
    "        MSE_test[degree] = np.mean( np.mean((z_test - z_hat_tests)**2, axis=1, keepdims=True ))\n",
    "        MSE_train[degree] = np.mean( np.mean((z_train - z_hat_trains)**2, axis=1, keepdims=True ))\n",
    "        bias[degree] = np.mean( (z_test - np.mean(z_hat_tests, axis=1, keepdims=True))**2 )\n",
    "        variance[degree] = np.mean( np.var(z_hat_tests, axis=1, keepdims=True))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(polydegree, MSE_test,\"m\", label='MSE test')\n",
    "    plt.plot(polydegree, MSE_train,\"c\", label='MSE train')\n",
    "\n",
    "    plt.plot(polydegree, bias,\"g--\", label='bias')\n",
    "    plt.plot(polydegree, variance,\"r--\", label='Variance')\n",
    "\n",
    "    plt.title(f\"Bias-Variance for lambda:{lmb}\")\n",
    "    plt.xlabel(\"Model complexity / Polynomial Degree\")\n",
    "    plt.ylabel(\"Prediction Error\")\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{REPORT_FIGURES}{EX6_4}Ridge_finding_optimal_lambda_using_bootstrap_lambda_num{i}.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "    summary_df, _ = model.summary()\n",
    "    CI_lower_betas = summary_df[\"CI lower\"]\n",
    "    CI_upper_betas = summary_df[\"CI upper\"]\n",
    "    betas = summary_df[\"coeff value\"]\n",
    "    i+=1\n",
    "\n",
    "    #display(model.summary())\n",
    "      "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ploting beta error for different lambda values for the most optimal degree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimal_degree = 4\n",
    "min_lambda = -4 \n",
    "max_lambda = 4 \n",
    "nbf_lambdas = 20\n",
    "lambdas = np.logspace(min_lambda,max_lambda, nbf_lambdas)\n",
    "summaries_df = pd.DataFrame()\n",
    "\n",
    "for lmb in lambdas:\n",
    "    #MSE_test = np.zeros(maxdegree)\n",
    "    #MSE_train = np.zeros(maxdegree)\n",
    "    #polydegree = np.arange(maxdegree)\n",
    "    #bias = np.zeros(maxdegree)\n",
    "    #variance = np.zeros(maxdegree)\n",
    "\n",
    "    X = create_X(x, y, n=optimal_degree)\n",
    "    X_train, X_test, z_train, z_test = prepare_data(X, z_flat, test_size=0.2, shuffle=True, scale_X=True, scale_t=True, random_state=SEED_VALUE)\n",
    "    model =  RidgeRegression(lambda_val=lmb)\n",
    "    z_hat_train = model.fit(X_train, z_train, SVDfit=False, keep_intercept=False)        \n",
    "    summary_df = model.summary()\n",
    "    summaries_df = pd.concat([summaries_df,summary_df], axis=0)\n",
    "    \n",
    "fig = plot_beta_errors_for_lambdas(summaries_df, optimal_degree)\n",
    "fig.savefig(f\"{REPORT_FIGURES}{EX6_4}Ridge_beta_SE_for_lambdas.pdf\")\n",
    "\n",
    "fig = plot_beta_CI_for_lambdas(summaries_df, optimal_degree)\n",
    "fig.savefig(f\"{REPORT_FIGURES}{EX6_4}Ridge_beta_CI_for_lambdas.pdf\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.4.4 Ridge regression - finding optimal lambda using cross-validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.4.5 Ridge regression - bias-variance trade-off using bootstrap"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.5. Lasso Regression on the Franke function with resampling (Exercise5)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.5.1 Lasso"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tests of dimensions image reduction and patches"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "i = 1\n",
    "value = 720\n",
    "while True:\n",
    "    i += 1\n",
    "    if((value % i)==0):\n",
    "        print(f\"value:{value / i} at i:{i}\")\n",
    "    if(i>=value):\n",
    "        break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "i = 1\n",
    "value = 360\n",
    "while True:\n",
    "    i += 1\n",
    "    if((value % i)==0):\n",
    "        print(f\"value:{value / i} at i:{i}\")\n",
    "    if(i>=value):\n",
    "        break"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}