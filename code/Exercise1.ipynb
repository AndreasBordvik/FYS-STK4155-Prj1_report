{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import seaborn as sns\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "import os\r\n",
    "from common import *\r\n",
    "import pandas as pd\r\n",
    "#from mpl_toolkits.mplot3d import Axes3D\r\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\r\n",
    "\r\n",
    "os.chdir(\"../\")\r\n",
    "print(f\"Root directory: {os.getcwd()}\")\r\n",
    "\r\n",
    "plt.rcParams.update({\r\n",
    "    \"text.usetex\": True,\r\n",
    "    \"font.family\": \"serif\",\r\n",
    "    \"font.serif\": [\"Palatino\"],\r\n",
    "    \"font.size\": 10,\r\n",
    "})\r\n",
    "\r\n",
    "%matplotlib inline "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Root directory: c:\\Users\\andre\\Dropbox\\FYS-STK4155_projects\\FYS-STK4155 - Project1\\FYS-STK4155-Prj1_report\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Global variables "
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "INPUT_DATA = \"data/input_data/\"  # Path for input data\r\n",
    "REPORT_DATA = \"data/report_data/\" # Path for data ment for the report\r\n",
    "REPORT_FIGURES = \"figures/\" # Path for figures ment for the report"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Franke function 3D preview\r\n",
    "First we plot a 3D plot of the franke function.\r\n",
    "The plot is based on the provided code in the assignmentext for plotting the franke function "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Preview plot of the franke function\r\n",
    "%matplotlib\r\n",
    "np.random.seed(4155)\r\n",
    "y = x = np.arange(0, 1, 0.05)\r\n",
    "x, y = np.meshgrid(x,y)\r\n",
    "z = FrankeFunction(x, y)\r\n",
    "noise_mesh = 0.05 * np.random.randn(z.shape[0], z.shape[1]) # Stochastic noise\r\n",
    "z_noisy = z + noise_mesh\r\n",
    "\r\n",
    "fig = plt.figure()\r\n",
    "# Ploting frankefunction without noise\r\n",
    "ax1 = fig.add_subplot(1,2,1, projection='3d') # Are :)steike\r\n",
    "ax1.title.set_text(\"Plot of the Franke Function\")\r\n",
    "ax1.view_init(elev=30., azim=-25.0)\r\n",
    "ax1.set_xlabel(\"x\"); ax1.set_ylabel(\"y\"); ax1.set_zlabel(\"z\")\r\n",
    "surf1 = ax1.plot_surface(x,y,z, cmap=cm.coolwarm, linewidth = 0, antialiased=False)\r\n",
    "# Customize the z axis.\r\n",
    "ax1.set_zlim(-0.10, 1.40)\r\n",
    "ax1.zaxis.set_major_locator(LinearLocator(10))\r\n",
    "ax1.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\r\n",
    "\r\n",
    "# Ploting frankefunction with noise\r\n",
    "ax2 = fig.add_subplot(1,2,2, projection='3d')\r\n",
    "ax2.title.set_text(\"Plot of the Franke Function\\n(noise added)\")\r\n",
    "ax2.view_init(elev=30., azim=-25.0)\r\n",
    "ax2.set_xlabel(\"x\"); ax2.set_ylabel(\"y\"); ax2.set_zlabel(\"z\")\r\n",
    "surf2 = ax2.plot_surface(x,y,z_noisy, cmap=cm.coolwarm, linewidth = 0, antialiased=False)\r\n",
    "# Customize the z axis.\r\n",
    "ax2.set_zlim(-0.10, 1.40)\r\n",
    "ax2.zaxis.set_major_locator(LinearLocator(10))\r\n",
    "ax2.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\r\n",
    "\r\n",
    "plt.savefig(f\"{REPORT_FIGURES}franke_function_preview.pdf\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 - Ordinary Least Squeares (OLS)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Data\r\n",
    "Defining and creating the data\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "np.random.seed(4155)\r\n",
    "n = 1000 # The number of point in the franke function\r\n",
    "x = np.random.rand(n)\r\n",
    "y = np.random.rand(n)\r\n",
    "z = FrankeFunction(x, y) \r\n",
    "#noise = 0.05 * np.random.randn(n) # Stochastic noise\r\n",
    "noise = 0.7 * np.random.randn(n) # Stochastic noise\r\n",
    "z += noise # Adding stochastic noise"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Plot of fit for all degrees before evaluation\r\n",
    " We plot the fit up to degree 6 to get an intuition on the curvature of the fitted models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = plt.figure(figsize=(8,8))\r\n",
    "degrees = 6\r\n",
    "z_train_OLS = pd.DataFrame()\r\n",
    "z_hat_train_OLS = pd.DataFrame()\r\n",
    "z_test_OLS = pd.DataFrame()\r\n",
    "z_hat_test_OLS = pd.DataFrame()\r\n",
    "\r\n",
    "# TODO: Must fix so that training and test data are used. \r\n",
    "# Must evalute model using MSE from traning and test\r\n",
    "for degree in range(1, degrees + 1):\r\n",
    "    X = create_X(x, y, degree) # Design MatrixS\r\n",
    "    model = OLS() # The model\r\n",
    "    model.fit(X, z) # Fitting the model\r\n",
    "    z_hat = model.predict(X) # predict on train data\r\n",
    "    \r\n",
    "    # Plot\r\n",
    "    ax = fig.add_subplot(3,2, degree, projection='3d')\r\n",
    "    ax.view_init(elev=30., azim=-25.0)\r\n",
    "    ax.title.set_text(f\"OLS/Linear fit of degree{degree}\")\r\n",
    "    ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_zlabel(\"z\")\r\n",
    "    ax.scatter3D(y, x, z_hat, c=z_hat ,marker = '.', cmap=cm.coolwarm)\r\n",
    "fig.suptitle(\"OLS fit to the Franke Function\")\r\n",
    "plt.tight_layout()\r\n",
    "plt.savefig(f\"{REPORT_FIGURES}franke_function_OLS_fit.pdf\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 - Finding degree/model complexity for the optimal OLS fit\r\n",
    "Approximate the franke function using ordinary least squares\r\n",
    "We estimate the franke functinon using polynomials up to 6th degree. We than look at the MSE scores to look for overfitting. We use the MSE score values from the test data to determine overfit together with the curvature of the evaluation plot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Confidence intervall \r\n",
    "$$CI_{0.95}(\\hat\\beta_i) = [\\hat\\beta_i-1.96 SE(\\hat\\beta_i), \\hat\\beta_i+1.96 SE(\\hat\\beta_i)] =\\hat\\beta_i \\pm 1.96\\hat \\sigma(\\hat\\beta_i)$$ \r\n",
    "In order to estimate the variance of the $i$-th beta values: $$\\sigma^2 (\\beta_i ) = \\sigma^2 [(X^{T} X)^{-1}]_{ii}$$\r\n",
    "However, $\\sigma$ is unkown and can be generaly estimated as followed:\r\n",
    "$$\\hat\\sigma^2 = \\frac{\\sum_{i=0}^{N-1}(y_i - \\hat y_i)^2}{N}$$\r\n",
    "For simplification purposes, we N instead of N-p-1 in the denominator.<br>\r\n",
    "To get the variance estimate of each $\\beta$ component one must calculate the variance with respect to the diagonal elements of $(X^TX)^{-1}$ Estimated standard error is the square root of $\\hat\\sigma^2$, where the estimate for variance $\\hat\\sigma^2$ is:\r\n",
    "$$\\hat\\sigma^2 = \\frac{\\sum_{i=0}^{N-1}(y_i - \\hat y_i)^2}{N}(X^TX)^{-1}$$\r\n",
    "Where y is the true value, and $\\hat y$ being the predicted value. <br>\r\n",
    "The variance estimate of each $\\hat\\beta$ estimate can be written as:\r\n",
    "$$\\hat\\sigma_{\\hat\\beta_i}^2 = \\hat\\sigma^2(X^TX)_{i,i}^{-1}$$\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "degrees = 2\r\n",
    "z_train_OLS = pd.DataFrame()\r\n",
    "z_hat_train_OLS = pd.DataFrame()\r\n",
    "z_test_OLS = pd.DataFrame()\r\n",
    "z_hat_test_OLS = pd.DataFrame()\r\n",
    "\r\n",
    "# TODO: Must fix so that training and test data are used. \r\n",
    "# Must evalute model using MSE from traning and test\r\n",
    "conf_df = pd.DataFrame()\r\n",
    "\r\n",
    "for degree in range(1, degrees):\r\n",
    "    X = create_X(x, y, degree) # Design Matrix\r\n",
    "    #X_train, X_test, z_train, z_test = prepare_data(X, z, test_size=0.2, shuffle=True, scale_X=False, scale_t=False)\r\n",
    "    X_train, X_test, z_train, z_test = prepare_data(X, z, test_size=0.2, shuffle=True, scale_X=False, scale_t=False, random_state=4155)\r\n",
    "    model = OLS() # The model\r\n",
    "    X_train = X; z_train = z\r\n",
    "    model.fit(X_train, z_train) # Fitting the model\r\n",
    "    z_hat_train = model.predict(X_train) # predict on train data\r\n",
    "    z_hat_test = model.predict(X_test) # predict on test data\r\n",
    "    \r\n",
    "\r\n",
    "   \r\n",
    "\r\n",
    "    # Evaluatation metrics\r\n",
    "    # TODO:\r\n",
    "    MSE_score = MSE(z_train, z_hat_train)\r\n",
    "    R2_score = R2(z_train, z_hat_train)\r\n",
    "    var_z_train = np.var(z_train)\r\n",
    "\r\n",
    "    N, P = X.shape\r\n",
    "    #sigma_hat = np.sqrt(np.sum((z - z_hat_train)**2)) / N\r\n",
    "    var_hat = (1/(N-P-1)) * np.sum((z - z_hat_train)**2)\r\n",
    "    sigma_hat = np.sqrt(var_hat)\r\n",
    "\r\n",
    "    #invXTX_diag = np.diag(np.linalg.pinv(X_train.T @ X_train)); print(\"Diag\", np.sqrt(invXTX_diag))\r\n",
    "    invXTX_diag = np.diag(np.linalg.inv(X_train.T @ X_train)); print(\"Diag\", np.sqrt(invXTX_diag))\r\n",
    "    SE_betas = var_hat * invXTX_diag\r\n",
    "    SE_betas = np.sqrt(SE_betas)\r\n",
    "    \r\n",
    "    #variance = np.sum(((z_train - z_hat_train) ** 2)) / N\r\n",
    "    #sigma = np.sqrt(variance)\r\n",
    "    \r\n",
    "    #SE_betas = np.sqrt(variance * invXTX_diag)     \r\n",
    "    #SE_betas = sigma * np.sqrt(invXTX_diag)     \r\n",
    "    \r\n",
    "    betas = model.get_all_betas()\r\n",
    "    CI_lower_all_betas = betas - (1.96 * SE_betas)\r\n",
    "    CI_upper_all_betas = betas + (1.96 * SE_betas)\r\n",
    "    CL = np.zeros((CI_upper_all_betas.shape[0],2))\r\n",
    "    CL[:,0] = CI_lower_all_betas\r\n",
    "    CL[:,1] = CI_upper_all_betas\r\n",
    "\r\n",
    "    #df = pd.DataFrame(columns=[\"lower_conf\",\"upper_conf\", \"beta coefficient\" ])\r\n",
    "    #print(\"CI shape\",CI_lower_all_betas.shape)\r\n",
    "    #print(degree*betas.shape)\r\n",
    "    #df = pd.DataFrame.from_dict({\"lower_conf\":CI_lower_all_betas, \"upper_conf\":CI_upper_all_betas, \r\n",
    "    #                             \"beta\": betas,  \"degree\" :degree*betas.shape}, orient='index').T\r\n",
    "    #conf_df = pd.concat([conf_df,df], axis=0)\r\n",
    "\r\n",
    "    \r\n",
    "    print(f\"Degree: {degree}\")\r\n",
    "    print(f\"Mean Square Error: {MSE_score}\")\r\n",
    "    print(f\"R2 score: {R2_score}\")\r\n",
    "    print(f\"Variance: {var_z_train}\")\r\n",
    "    print(f\"Sigma:{sigma_hat}\")\r\n",
    "    print(\"BetaSigma: \")\r\n",
    "    print(SE_betas)\r\n",
    "    print(\"ConfidenceInterval: \")\r\n",
    "    print(CL)\r\n",
    "    print(\"Beta: \")\r\n",
    "    print(betas)\r\n",
    "    #print(\"Relative confidense: \")\r\n",
    "    #print(relative)\r\n",
    "    #\"beta_no\": f\"beta{np.arange(betas.shape[0]+1)}\",\r\n",
    "\r\n",
    "    print(z.shape)\r\n",
    "    print(X.shape)\r\n",
    "    import statsmodels.api as sm\r\n",
    "    mod = sm.OLS(np.expand_dims(z_train, axis=1),X_train).fit()\r\n",
    "    print(mod.summary())\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    conf_intervall_betas = np.zeros((betas.shape[0],2))\r\n",
    "    for i in range(betas):\r\n",
    "        conf_intervall_betas[i, 0] = beta - 1.966 * SE_betas[i]\r\n",
    "\r\n",
    "\r\n",
    "    #variance = np.sum((z_train - np.mean(z_hat_train)) ** 2) / n\r\n",
    "    #sigma = np.sqrt(variance)\r\n",
    "    #sigma = np.sqrt(1/(X.shape[0] - X.shape[1] - 1) * np.sum((z_test - z_hat_test)**2))\r\n",
    "    \r\n",
    "    \r\n",
    "    #variance_estimate = sigma * X_T_X_diag\r\n",
    "    #variance_estimate = sigma * np.sqrt(X_T_X_diag) ## sqrt of X_T_X_diag???\r\n",
    "\r\n",
    "    for i, beta in enumerate(betas):\r\n",
    "        \r\n",
    "        #var_beta = np.diag(sigma *np.linalg.inv((X.T.dot(X)))) # Variance of the beta coefficients\r\n",
    "        #variance_estimate_beta_i = sigma * np.sqrt(np.linalg.pinv(X.T @ X))[i,i] # Variance of the beta coefficients\r\n",
    "        \r\n",
    "        \r\n",
    "        SE_beta_i = np.sqrt(variance_estimate_beta_i / N) # Standard error of estimated beta\r\n",
    "        lower = beta - 1.966 * SE_beta_i\r\n",
    "        upper = beta + 1.966 * SE_beta_i\r\n",
    "        df = pd.DataFrame.from_dict({\"lower_conf\":lower, \"upper_conf\":upper, \"beta\": beta, \"beta_no\": f\"beta{i+1}\", \"degree\" :degree}, orient='index').T\r\n",
    "        conf_df = pd.concat([conf_df,df], axis=0)\r\n",
    "\r\n",
    "        \r\n",
    "        #error = np.mean( np.mean((y_test - y_pred)**2, axis=1, keepdims=True) )\r\n",
    "        #bias = np.mean( (y_test - np.mean(y_pred, axis=1, keepdims=True))**2 )\r\n",
    "        #variance = np.mean( np.var(y_pred, axis=1, keepdims=True) )\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # Filling up dataframes\r\n",
    "    z_train_OLS[degree] = z_train.flatten() \r\n",
    "    z_hat_train_OLS[degree] = z_hat_train.flatten()\r\n",
    "    z_test_OLS[degree] = z_test.flatten()\r\n",
    "    z_hat_test_OLS[degree] = z_hat_test.flatten()\r\n",
    "\r\n",
    "\r\n",
    "# MSE calculations for all lambda values\r\n",
    "mse_scores_train = ((z_train_OLS - z_hat_train_OLS) ** 2).mean()\r\n",
    "mse_scores_test = ((z_test_OLS - z_hat_test_OLS) ** 2).mean()\r\n",
    "# R2 calculations for all lambda values\r\n",
    "R2_scores_train = 1 - ((z_train_OLS - z_hat_train_OLS) ** 2).sum() / ((z_train_OLS - z_train_OLS.mean())**2).sum() \r\n",
    "R2_scores_test = 1 - ((z_test_OLS - z_hat_test_OLS) ** 2).sum() / ((z_test_OLS - z_test_OLS.mean())**2).sum()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Diag [0.08164447 0.10757432 0.10839043]\n",
      "Degree: 1\n",
      "Mean Square Error: 0.5783645675175659\n",
      "R2 score: 0.014245859021887686\n",
      "Variance: 0.5867229398029056\n",
      "Sigma:0.7620284225569967\n",
      "BetaSigma: \n",
      "[0.06221541 0.08197469 0.08259659]\n",
      "ConfidenceInterval: \n",
      "[[ 0.57050611  0.81439051]\n",
      " [-0.39703062 -0.07568984]\n",
      " [-0.78415197 -0.46037333]]\n",
      "Beta: \n",
      "[ 0.69244831 -0.23636023 -0.62226265]\n",
      "(1000,)\n",
      "(1000, 3)\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.073\n",
      "Model:                            OLS   Adj. R-squared:                  0.071\n",
      "Method:                 Least Squares   F-statistic:                     39.05\n",
      "Date:                Tue, 21 Sep 2021   Prob (F-statistic):           4.70e-17\n",
      "Time:                        20:20:00   Log-Likelihood:                -1114.6\n",
      "No. Observations:                1000   AIC:                             2235.\n",
      "Df Residuals:                     997   BIC:                             2250.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.9257      0.060     15.348      0.000       0.807       1.044\n",
      "x1            -0.5066      0.079     -6.375      0.000      -0.663      -0.351\n",
      "x2            -0.4958      0.080     -6.192      0.000      -0.653      -0.339\n",
      "==============================================================================\n",
      "Omnibus:                        0.448   Durbin-Watson:                   2.090\n",
      "Prob(Omnibus):                  0.799   Jarque-Bera (JB):                0.471\n",
      "Skew:                           0.051   Prob(JB):                        0.790\n",
      "Kurtosis:                       2.971   Cond. No.                         5.18\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plots\r\n",
    "plt.figure(figsize=(12,8))\r\n",
    "plt.plot(np.arange(1,degrees), mse_scores_train, c=\"c\", label=\"Training data\")\r\n",
    "plt.plot(np.arange(1,degrees), mse_scores_test, c=\"m\", label=\"Test data\")\r\n",
    "plt.xlabel(\"Model complexity - Number of polynomial degrees\")\r\n",
    "plt.ylabel(\"MSE\")\r\n",
    "plt.title(\"Training evaluation on OLS regression fit\")\r\n",
    "plt.legend()\r\n",
    "plt.tight_layout()\r\n",
    "plt.savefig(f\"{REPORT_FIGURES}franke_function_OLS_evaluate_fit.pdf\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 Analysis of plots and training metrics\r\n",
    "Do the analysis.......\r\n",
    "\r\n",
    "Conclusion:<br>\r\n",
    "Based on the analysis, we conclude that a model complexity of degree 5 yields the most optimal fit."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.5 plot of the model using the most optimal parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "degree_optimal = 5\r\n",
    "X = create_X(x, y, degree_optimal) # Design Matrix\r\n",
    "model = OLS() # The model\r\n",
    "model.fit(X, z) # Fitting the model\r\n",
    "z_hat = model.predict(X) # predict on train data\r\n",
    "\r\n",
    "# Plot\r\n",
    "fig = plt.figure()#figsize=(8,8))\r\n",
    "ax = plt.axes(projection='3d')\r\n",
    "ax.title.set_text(f\"OLS regression fit to the Franke Function\\noptimal degree {degree_optimal},\")\r\n",
    "#ax.view_init(elev=5., azim=85.0)\r\n",
    "ax.view_init(elev=30., azim=-25.0)\r\n",
    "ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_zlabel(\"z\")\r\n",
    "ax.scatter3D(y, x, z_hat, c=z_hat ,marker = '.', cmap=cm.coolwarm)\r\n",
    "plt.savefig(f\"{REPORT_FIGURES}franke_function_OLS_best_fit.pdf\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('pytorch_lightning': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "interpreter": {
   "hash": "cdd16a4e159c1067d9dfa51729b478ccc85a4bb59359633d71fef71fb1a46b1f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}