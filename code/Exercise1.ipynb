{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from common import *\n",
    "import pandas as pd\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "import sklearn.linear_model as lm\n",
    "\n",
    "print(f\"Root directory: {os.getcwd()}\")\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "    \"font.size\": 10,\n",
    "})\n",
    "\n",
    "#%matplotlib inline "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Global variables "
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Franke function 3D preview\r\n",
    "First we plot a 3D plot of the franke function.\r\n",
    "The plot is based on the provided code in the assignmentext for plotting the franke function "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Preview plot of the franke function\n",
    "#%matplotlib\n",
    "np.random.seed(4155)\n",
    "y = x = np.arange(0, 1, 0.05)\n",
    "x, y = np.meshgrid(x,y)\n",
    "z = FrankeFunction(x, y)\n",
    "noise_mesh = 0.05 * np.random.randn(z.shape[0], z.shape[1]) # Stochastic noise\n",
    "z_noisy = z + noise_mesh\n",
    "\n",
    "fig = plt.figure()\n",
    "# Ploting frankefunction without noise\n",
    "ax1 = fig.add_subplot(1,2,1, projection='3d') # Are :)steike\n",
    "ax1.title.set_text(\"Plot of the Franke Function\")\n",
    "ax1.view_init(elev=30., azim=-25.0)\n",
    "ax1.set_xlabel(\"x\"); ax1.set_ylabel(\"y\"); ax1.set_zlabel(\"z\")\n",
    "surf1 = ax1.plot_surface(x,y,z, cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n",
    "# Customize the z axis.\n",
    "ax1.set_zlim(-0.10, 1.40)\n",
    "ax1.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax1.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "# Ploting frankefunction with noise\n",
    "ax2 = fig.add_subplot(1,2,2, projection='3d')\n",
    "ax2.title.set_text(\"Plot of the Franke Function\\n(noise added)\")\n",
    "ax2.view_init(elev=30., azim=-25.0)\n",
    "ax2.set_xlabel(\"x\"); ax2.set_ylabel(\"y\"); ax2.set_zlabel(\"z\")\n",
    "surf2 = ax2.plot_surface(x,y,z_noisy, cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n",
    "# Customize the z axis.\n",
    "ax2.set_zlim(-0.10, 1.40)\n",
    "ax2.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax2.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "plt.savefig(f\"{REPORT_FIGURES}franke_function_preview.pdf\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 - Ordinary Least Squeares (OLS)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Data\r\n",
    "Defining and creating the data\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.random.seed(4155)\n",
    "n = 100 # The number of point in the franke function\n",
    "x = np.random.rand(n)\n",
    "y = np.random.rand(n)\n",
    "x, y = np.meshgrid(x,y)\n",
    "z = FrankeFunction(x, y) \n",
    "noise = 0.5 * np.random.randn(n,n) # Stochastic noise\n",
    "#noise = 0.7 * np.random.randn(n) # Stochastic noise\n",
    "z_noise = z + noise # separate Franke with noise\n",
    "#z += noise # Adding stochastic noise"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Plot of fit for all degrees before evaluation\r\n",
    " We plot the fit up to degree 6 to get an intuition on the curvature of the fitted models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "degrees = 6\n",
    "z_train_OLS = pd.DataFrame()\n",
    "z_hat_train_OLS = pd.DataFrame()\n",
    "z_test_OLS = pd.DataFrame()\n",
    "z_hat_test_OLS = pd.DataFrame()\n",
    "\n",
    "# TODO: Must fix so that training and test data are used. \n",
    "# Must evalute model using MSE from traning and test\n",
    "for degree in range(1, degrees + 1):\n",
    "    X = create_X(x, y, degree) # Design MatrixS\n",
    "    model = OLS() # The model\n",
    "    #model.fit(X, z) # Fitting the model\n",
    "    model.fit(X, z.ravel()) # Fitting the model\n",
    "    z_hat = model.predict(X) # predict on train data\n",
    "    \n",
    "    # Plot\n",
    "    ax = fig.add_subplot(3,2, degree, projection='3d')\n",
    "    ax.view_init(elev=30., azim=-25.0)\n",
    "    ax.title.set_text(f\"OLS/Linear fit of degree{degree}\")\n",
    "    ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_zlabel(\"z\")\n",
    "    ax.scatter3D(y, x, z_hat, c=z_hat ,marker = '.', cmap=cm.coolwarm)\n",
    "fig.suptitle(\"OLS fit to the Franke Function\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{REPORT_FIGURES}franke_function_OLS_fit.pdf\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 - Finding degree/model complexity for the optimal OLS fit\r\n",
    "Approximate the franke function using ordinary least squares\r\n",
    "We estimate the franke functinon using polynomials up to 6th degree. We than look at the MSE scores to look for overfitting. We use the MSE score values from the test data to determine overfit together with the curvature of the evaluation plot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Confidence intervall \r\n",
    "$$CI_{0.95}(\\hat\\beta_i) = [\\hat\\beta_i-1.96 SE(\\hat\\beta_i), \\hat\\beta_i+1.96 SE(\\hat\\beta_i)] =\\hat\\beta_i \\pm 1.96\\hat \\sigma(\\hat\\beta_i)$$ \r\n",
    "In order to estimate the variance of the $i$-th beta values: $$\\sigma^2 (\\beta_i ) = \\sigma^2 [(X^{T} X)^{-1}]_{ii}$$\r\n",
    "However, $\\sigma$ is unkown and can be generaly estimated as followed:\r\n",
    "$$\\hat\\sigma^2 = \\frac{\\sum_{i=0}^{N-1}(y_i - \\hat y_i)^2}{N}$$\r\n",
    "For simplification purposes, we N instead of N-p-1 in the denominator.<br>\r\n",
    "To get the variance estimate of each $\\beta$ component one must calculate the variance with respect to the diagonal elements of $(X^TX)^{-1}$ Estimated standard error is the square root of $\\hat\\sigma^2$, where the estimate for variance $\\hat\\sigma^2$ is:\r\n",
    "$$\\hat\\sigma^2 = \\frac{\\sum_{i=0}^{N-1}(y_i - \\hat y_i)^2}{N}(X^TX)^{-1}$$\r\n",
    "Where y is the true value, and $\\hat y$ being the predicted value. <br>\r\n",
    "The variance estimate of each $\\hat\\beta$ estimate can be written as:\r\n",
    "$$\\hat\\sigma_{\\hat\\beta_i}^2 = \\hat\\sigma^2(X^TX)_{i,i}^{-1}$$\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "degrees = 5\n",
    "\n",
    "# Setting up dataframes for the observed values\n",
    "z_train_OLS = pd.DataFrame()\n",
    "z_test_OLS = pd.DataFrame()\n",
    "\n",
    "# Setting up the dataframes for our computed values\n",
    "z_hat_train_OLS = pd.DataFrame()\n",
    "z_hat_test_OLS = pd.DataFrame()\n",
    "\n",
    "# Setting up the dataframes for our SVD computed values\n",
    "z_hat_train_SVD = pd.DataFrame()\n",
    "z_hat_test_SVD = pd.DataFrame()\n",
    "\n",
    "# Setting up dataframes for sklearn computed values\n",
    "z_hat_train_SK = pd.DataFrame()\n",
    "z_hat_test_SK = pd.DataFrame()\n",
    "\n",
    "coeffs_df = pd.DataFrame()\n",
    "\n",
    "for degree in range(1, degrees+1):\n",
    "    X = create_X(x, y, degree) # Design Matrix\n",
    "    \n",
    "    # Scaling data and splitting it into training and test sets\n",
    "    #X_train, X_test, z_train, z_test = prepare_data(X, z, test_size=0.2, shuffle=True, scale_X=False, scale_t=False)\n",
    "    X_train, X_test, z_train, z_test = prepare_data(X, z.ravel(), test_size=0.2, shuffle=True, scale_X=False, scale_t=False, random_state=4155)\n",
    "    \n",
    "    # Model construction, fitting, and predictions using matrix inversion\n",
    "    model = OLS() # The model\n",
    "    z_hat_train = model.fit(X_train, z_train, SVDfit = False) # Fitting the model and predict on training data\n",
    "    z_hat_test = model.predict(X_test) # predict on test data\n",
    "\n",
    "    # Model construction, fitting and predictions using SVD inversion\n",
    "    model_svd = OLS()\n",
    "    z_hat_train_svd = model_svd.fit(X_train, z_train)\n",
    "    z_hat_test_svd = model.predict(X_test)\n",
    "\n",
    "    # Model construction, fitting and predictions using sklearn\n",
    "    model_sk = lm.LinearRegression()\n",
    "    model_sk.fit(X_train, z_train)\n",
    "    z_hat_train_sk = model_sk.predict(X_train)\n",
    "    z_hat_test_sk = model_sk.predict(X_test)\n",
    "    \n",
    "    # Evaluatation metrics OLS, SVD, SK\n",
    "    MSE_score_train = MSE(z_train, z_hat_train)\n",
    "    R2_score_train = R2(z_train, z_hat_train)\n",
    "    MSE_score_test = MSE(z_test, z_hat_test)\n",
    "    R2_score_test = R2(z_test, z_hat_test)\n",
    "\n",
    "    MSE_score_train_svd = MSE(z_train, z_hat_train_svd)\n",
    "    R2_score_train_svd = R2(z_train, z_hat_train_svd)\n",
    "    MSE_score_test_svd = MSE(z_test, z_hat_test_svd)\n",
    "    R2_score_test_svd = R2(z_test, z_hat_test_svd)\n",
    "\n",
    "    MSE_score_train_sk = MSE(z_train, z_hat_train_sk)\n",
    "    R2_score_train_sk = R2(z_train, z_hat_train_sk)\n",
    "    MSE_score_test_sk = MSE(z_test, z_hat_test_sk)\n",
    "    R2_score_test_sk = R2(z_test, z_hat_test_sk)\n",
    "    \n",
    "    # Estimated standard error for the beta coefficients\n",
    "    SE_betas = model.SE\n",
    "    SE_betas_svd = model_svd.SE\n",
    "\n",
    "    #N, P = X_train.shape\n",
    "    #var_hat = (1/(N-P-1)) * np.sum((z_train - z_hat_train)**2)\n",
    "    var_hat = (1/X_train.shape[0]) * np.sum((z_train - z_hat_train)**2) # Estimated variance\n",
    "    #invXTX_diag = np.diag(np.linalg.inv(X_train.T @ X_train)) \n",
    "    #SE_betas = np.sqrt(var_hat * invXTX_diag) # Standard Error\n",
    "\n",
    "    # Calculating 95% confidence intervall OLS, SVD\n",
    "    betas = model.get_all_betas\n",
    "    CI_lower_all_betas = betas - (1.96 * SE_betas)\n",
    "    CI_upper_all_betas = betas + (1.96 * SE_betas)\n",
    "    CL = np.zeros((CI_upper_all_betas.shape[0],2))\n",
    "    CL[:,0] = CI_lower_all_betas\n",
    "    CL[:,1] = CI_upper_all_betas\n",
    "\n",
    "    betas_svd = model.get_all_betas\n",
    "\n",
    "        \n",
    "    # Printing results\n",
    "    print(f\"Degree: {degree}\")\n",
    "    print(f\"Train data - Mean Square Error: {MSE_score_train}\")\n",
    "    print(f\"Train data - R2 score: {R2_score_train}\")\n",
    "    print(f\"Test data - Mean Square Error: {MSE_score_test}\")\n",
    "    print(f\"Test data - R2 score: {R2_score_test}\")\n",
    "    print(f\"Estimated variance: {var_hat}\")\n",
    "    print(f\"Beta - values{betas}\")\n",
    "    print(f\"Beta - Std Errors: {SE_betas}\")\n",
    "    print(f\"Beta - Confidence Interval (CI):\\n{CL}\\n\")\n",
    "\n",
    "    # Constructing dataframe for beta coefficients\n",
    "    degs = np.zeros(betas.shape[0]); degs.fill(degree)\n",
    "    df = pd.DataFrame.from_dict({\"degree\" :degs,\n",
    "                                 \"coeff name\": [f\"b{i}\" for i in range(1,betas.shape[0]+1)],\n",
    "                                 \"coeff value\": np.round(betas, decimals=4),\n",
    "                                 \"Std Error\": np.round(SE_betas, decimals=4),\n",
    "                                 \"CI lower\":np.round(CI_lower_all_betas, decimals=4), \n",
    "                                 \"CI_upper\":np.round(CI_upper_all_betas, decimals=4)},\n",
    "                                 orient='index').T\n",
    "    coeffs_df = pd.concat([coeffs_df,df], axis=0)\n",
    "        \n",
    "    # Filling up dataframes for train and test evaluation\n",
    "    z_train_OLS[degree] = z_train.flatten() \n",
    "    z_hat_train_OLS[degree] = z_hat_train.flatten()\n",
    "    z_test_OLS[degree] = z_test.flatten()\n",
    "    z_hat_test_OLS[degree] = z_hat_test.flatten()\n",
    "\n",
    "    z_hat_train_SVD[degree] = z_hat_train_svd.flatten()\n",
    "    z_hat_test_SVD[degree] = z_hat_test_svd.flatten()\n",
    "\n",
    "    z_hat_train_SK[degree] = z_hat_train_sk.flatten()\n",
    "    z_hat_test_SK[degree] = z_hat_test_sk.flatten()\n",
    "\n",
    "\n",
    "# MSE calculations for all lambda values\n",
    "mse_scores_train = ((z_train_OLS - z_hat_train_OLS) ** 2).mean()\n",
    "mse_scores_test = ((z_test_OLS - z_hat_test_OLS) ** 2).mean()\n",
    "\n",
    "mse_scores_train_svd = ((z_train_OLS - z_hat_train_SVD) ** 2).mean()\n",
    "mse_scores_test_svd = ((z_test_OLS - z_hat_test_SVD) ** 2).mean()\n",
    "\n",
    "mse_scores_train_sk = ((z_train_OLS - z_hat_train_SK) ** 2).mean()\n",
    "mse_scores_test_sk = ((z_test_OLS - z_hat_test_SK) ** 2).mean()\n",
    "\n",
    "# R2 calculations for all lambda values\n",
    "R2_scores_train = 1 - ((z_train_OLS - z_hat_train_OLS) ** 2).sum() / ((z_train_OLS - z_train_OLS.mean())**2).sum() \n",
    "R2_scores_test = 1 - ((z_test_OLS - z_hat_test_OLS) ** 2).sum() / ((z_test_OLS - z_test_OLS.mean())**2).sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plots\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_train, c=\"c\", label=\"Training data\")\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_test, c=\"m\", label=\"Test data\")\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_train_svd, \"b--\", label=\"Training data SVD\")\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_test_svd, \"g--\", label=\"Test data SVD\")\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_train_sk, \"r--\", label=\"Training data sklearn\")\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_test_sk, \"y--\", label=\"Test data sklearn\")\n",
    "plt.xlabel(\"Model complexity / Polynomial Degree\")\n",
    "plt.ylabel(\"Prediction Error - MSE\")\n",
    "plt.title(\"Training evaluation on OLS regression fit\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{REPORT_FIGURES}franke_function_OLS_evaluate_fit.pdf\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 Analysis of plots and training metrics\r\n",
    "Do the analysis.......\r\n",
    "\r\n",
    "Conclusion:<br>\r\n",
    "Based on the analysis, we conclude that a model complexity of degree 5 yields the most optimal fit."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.5 plot of the model using the most optimal parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimal_degree = 5\n",
    "X = create_X(x, y, optimal_degree) # Design Matrix\n",
    "model = OLS() # The model\n",
    "model.fit(X, z.ravel()) # Fitting the model\n",
    "z_hat = model.predict(X) # predict on train data\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure()#figsize=(8,8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.title.set_text(f\"OLS regression fit to the Franke Function\\noptimal degree {optimal_degree},\")\n",
    "#ax.view_init(elev=5., azim=85.0)\n",
    "ax.view_init(elev=30., azim=-25.0)\n",
    "ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_zlabel(\"z\")\n",
    "ax.scatter3D(y, x, z_hat, c=z_hat ,marker = '.', cmap=cm.coolwarm)\n",
    "plt.savefig(f\"{REPORT_FIGURES}franke_function_OLS_best_fit.pdf\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimal_degree_coeffs = coeffs_df[coeffs_df[\"degree\"] == optimal_degree]\n",
    "display(optimal_degree_coeffs)\n",
    "optimal_degree_coeffs.to_csv(f\"{REPORT_DATA}EX1_coeffs_optimal_degree.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Repeat of the previous exercise, this time with scaled data to study the difference for OLS with and without scaled data\n",
    "(Hint: there should be no difference in MSE for OLS given scaled data)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setting up dataframes for the observed values\n",
    "z_train_OLS = pd.DataFrame()\n",
    "z_test_OLS = pd.DataFrame()\n",
    "\n",
    "# Setting up the dataframes for our computed values\n",
    "z_hat_train_OLS = pd.DataFrame()\n",
    "z_hat_test_OLS = pd.DataFrame()\n",
    "\n",
    "# Setting up the dataframes for our SVD computed values\n",
    "z_hat_train_SVD = pd.DataFrame()\n",
    "z_hat_test_SVD = pd.DataFrame()\n",
    "\n",
    "# Setting up dataframes for sklearn computed values\n",
    "z_hat_train_sk = pd.DataFrame()\n",
    "z_hat_test_sk = pd.DataFrame()\n",
    "\n",
    "coeffs_df = pd.DataFrame()\n",
    "\n",
    "for degree in range(1, degrees+1):\n",
    "    X = create_X(x, y, degree)[:,1:] # Design Matrix\n",
    "    \n",
    "    # Scaling data and splitting it into training and test sets\n",
    "    #X_train, X_test, z_train, z_test = prepare_data(X, z, test_size=0.2, shuffle=True, scale_X=False, scale_t=False)\n",
    "    X_train, X_test, z_train, z_test = prepare_data(X, z.ravel(), test_size=0.2, shuffle=True, scale_X=True, scale_t=True, zero_center = True, random_state=4155)\n",
    "    \n",
    "    # Model construction, fitting, and predictions using matrix inversion\n",
    "    model = OLS() # The model\n",
    "    z_hat_train = model.fit(X_train, z_train, SVDfit = False) # Fitting the model and predict on training data\n",
    "    z_hat_test = model.predict(X_test) # predict on test data\n",
    "\n",
    "    # Model construction, fitting and predictions using SVD inversion\n",
    "    model_svd = OLS()\n",
    "    z_hat_train_svd = model_svd.fit(X_train, z_train)\n",
    "    z_hat_test_svd = model.predict(X_test)\n",
    "\n",
    "    # Model construction, fitting and predictions using sklearn\n",
    "    model_sk = lm.LinearRegression(fit_intercept=False)\n",
    "    model_sk.fit(X_train, z_train)\n",
    "    z_hat_train_sk = model_sk.predict(X_train)\n",
    "    z_hat_test_sk = model_sk.predict(X_test)\n",
    "    \n",
    "    # Evaluatation metrics OLS, SVD, SK\n",
    "    MSE_score_train = MSE(z_train, z_hat_train)\n",
    "    R2_score_train = R2(z_train, z_hat_train)\n",
    "    MSE_score_test = MSE(z_test, z_hat_test)\n",
    "    R2_score_test = R2(z_test, z_hat_test)\n",
    "\n",
    "    MSE_score_train_svd = MSE(z_train, z_hat_train_svd)\n",
    "    R2_score_train_svd = R2(z_train, z_hat_train_svd)\n",
    "    MSE_score_test_svd = MSE(z_test, z_hat_test_svd)\n",
    "    R2_score_test_svd = R2(z_test, z_hat_test_svd)\n",
    "\n",
    "    MSE_score_train_sk = MSE(z_train, z_hat_train_sk)\n",
    "    R2_score_train_sk = R2(z_train, z_hat_train_sk)\n",
    "    MSE_score_test_sk = MSE(z_test, z_hat_test_sk)\n",
    "    R2_score_test_sk = R2(z_test, z_hat_test_sk)\n",
    "    \n",
    "    # Estimated standard error for the beta coefficients\n",
    "    SE_betas = model.SE\n",
    "    SE_betas_svd = model_svd.SE\n",
    "\n",
    "    #N, P = X_train.shape\n",
    "    #var_hat = (1/(N-P-1)) * np.sum((z_train - z_hat_train)**2)\n",
    "    var_hat = (1/X_train.shape[0]) * np.sum((z_train - z_hat_train)**2) # Estimated variance\n",
    "    #invXTX_diag = np.diag(np.linalg.inv(X_train.T @ X_train)) \n",
    "    #SE_betas = np.sqrt(var_hat * invXTX_diag) # Standard Error\n",
    "\n",
    "    # Calculating 95% confidence intervall OLS, SVD\n",
    "    betas = model.get_all_betas\n",
    "    CI_lower_all_betas = betas - (1.96 * SE_betas)\n",
    "    CI_upper_all_betas = betas + (1.96 * SE_betas)\n",
    "    CL = np.zeros((CI_upper_all_betas.shape[0],2))\n",
    "    CL[:,0] = CI_lower_all_betas\n",
    "    CL[:,1] = CI_upper_all_betas\n",
    "\n",
    "    betas_svd = model.get_all_betas\n",
    "\n",
    "        \n",
    "    # Printing results\n",
    "    print(f\"Degree: {degree}\")\n",
    "    print(f\"Train data - Mean Square Error: {MSE_score_train}\")\n",
    "    print(f\"Train data - R2 score: {R2_score_train}\")\n",
    "    print(f\"Test data - Mean Square Error: {MSE_score_test}\")\n",
    "    print(f\"Test data - R2 score: {R2_score_test}\")\n",
    "    print(f\"Estimated variance: {var_hat}\")\n",
    "    print(f\"Beta - values{betas}\")\n",
    "    print(f\"Beta - Std Errors: {SE_betas}\")\n",
    "    print(f\"Beta - Confidence Interval (CI):\\n{CL}\\n\")\n",
    "\n",
    "    # Constructing dataframe for beta coefficients\n",
    "    degs = np.zeros(betas.shape[0]); degs.fill(degree)\n",
    "    df = pd.DataFrame.from_dict({\"degree\" :degs,\n",
    "                                 \"coeff name\": [f\"b{i}\" for i in range(1,betas.shape[0]+1)],\n",
    "                                 \"coeff value\": np.round(betas, decimals=4),\n",
    "                                 \"Std Error\": np.round(SE_betas, decimals=4),\n",
    "                                 \"CI lower\":np.round(CI_lower_all_betas, decimals=4), \n",
    "                                 \"CI_upper\":np.round(CI_upper_all_betas, decimals=4)},\n",
    "                                 orient='index').T\n",
    "    coeffs_df = pd.concat([coeffs_df,df], axis=0)\n",
    "        \n",
    "    # Filling up dataframes for train and test evaluation\n",
    "    z_train_OLS[degree] = z_train.flatten() \n",
    "    z_hat_train_OLS[degree] = z_hat_train.flatten()\n",
    "    z_test_OLS[degree] = z_test.flatten()\n",
    "    z_hat_test_OLS[degree] = z_hat_test.flatten()\n",
    "\n",
    "    z_hat_train_SVD[degree] = z_hat_train_svd.flatten()\n",
    "    z_hat_test_SVD[degree] = z_hat_test_svd.flatten()\n",
    "\n",
    "\n",
    "# MSE calculations for all lambda values\n",
    "mse_scores_train = ((z_train_OLS - z_hat_train_OLS) ** 2).mean()\n",
    "mse_scores_test = ((z_test_OLS - z_hat_test_OLS) ** 2).mean()\n",
    "\n",
    "mse_scores_train_svd = ((z_train_OLS - z_hat_train_SVD) ** 2).mean()\n",
    "mse_scores_test_svd = ((z_test_OLS - z_hat_test_SVD) ** 2).mean()\n",
    "# R2 calculations for all lambda values\n",
    "#R2_scores_train = 1 - ((z_train_OLS - z_hat_train_OLS) ** 2).sum() / ((z_train_OLS - z_train_OLS.mean())**2).sum() \n",
    "#R2_scores_test = 1 - ((z_test_OLS - z_hat_test_OLS) ** 2).sum() / ((z_test_OLS - z_test_OLS.mean())**2).sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plots\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_train, c=\"c\", label=\"Training data\")\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_test, c=\"m\", label=\"Test data\")\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_train_svd, \"b--\", label=\"Training data SVD\")\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_test_svd, \"g--\", label=\"Test data SVD\")\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_train_sk, \"r--\", label=\"Training data sklearn\")\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_test_sk, \"y--\", label=\"Test data sklearn\")\n",
    "plt.xlabel(\"Model complexity / Polynomial Degree\")\n",
    "plt.ylabel(\"Prediction Error - MSE\")\n",
    "plt.title(\"Training evaluation on OLS regression fit given scaled data w.o. intercept\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{REPORT_FIGURES}franke_function_OLS_evaluate_fit.pdf\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# test of summary functnion\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimal_degree = 2\n",
    "X = create_X(x, y, optimal_degree) # Design Matrix\n",
    "model = OLS(optimal_degree) # The model\n",
    "z_hat_train = model.fit(X, z.ravel()) # Fitting the model & predict on train data\n",
    "summary = model.summary()\n",
    "display(summary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimal_lambda = -1\n",
    "X = create_X(x, y, 3) # Design Matrix\n",
    "model = RidgeRegression(optimal_lambda) # The model\n",
    "z_hat_train = model.fit(X, z.ravel()) # Fitting the model & predict on train data\n",
    "summary = model.summary()\n",
    "display(summary)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}