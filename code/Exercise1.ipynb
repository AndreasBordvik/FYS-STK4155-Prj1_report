{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import seaborn as sns\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "import os\r\n",
    "from common import *\r\n",
    "import pandas as pd\r\n",
    "#from mpl_toolkits.mplot3d import Axes3D\r\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\r\n",
    "\r\n",
    "os.chdir(\"../\")\r\n",
    "print(f\"Root directory: {os.getcwd()}\")\r\n",
    "\r\n",
    "plt.rcParams.update({\r\n",
    "    \"text.usetex\": True,\r\n",
    "    \"font.family\": \"serif\",\r\n",
    "    \"font.serif\": [\"Palatino\"],\r\n",
    "    \"font.size\": 10,\r\n",
    "})\r\n",
    "\r\n",
    "%matplotlib inline "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Root directory: c:\\Users\\andre\\Dropbox\\FYS-STK4155_projects\\FYS-STK4155 - Project1\\FYS-STK4155-Prj1_report\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Global variables "
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "INPUT_DATA = \"data/input_data/\"  # Path for input data\r\n",
    "REPORT_DATA = \"data/report_data/\" # Path for data ment for the report\r\n",
    "REPORT_FIGURES = \"figures/\" # Path for figures ment for the report"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First we plot a 3D plot of the franke function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Preview plot of the franke function\r\n",
    "%matplotlib\r\n",
    "np.random.seed(4155)\r\n",
    "y = x = np.arange(0, 1, 0.05)\r\n",
    "x, y = np.meshgrid(x,y)\r\n",
    "z = FrankeFunction(x, y)\r\n",
    "noise_mesh = 0.05 * np.random.randn(z.shape[0], z.shape[1])\r\n",
    "z_noisy = z + noise_mesh\r\n",
    "\r\n",
    "fig = plt.figure()\r\n",
    "ax = fig.add_subplot(1,2,1, projection='3d') # Are :)steike\r\n",
    "\r\n",
    "#ax = plt.axes(projection='3d')\r\n",
    "ax.title.set_text(\"Plot of the Franke Function\")\r\n",
    "ax.view_init(elev=30., azim=-25.0)\r\n",
    "ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_zlabel(\"z\")\r\n",
    "surf = ax.plot_surface(x,y,z, cmap=cm.coolwarm, linewidth = 0, antialiased=False)\r\n",
    "\r\n",
    "ax = fig.add_subplot(1,2,2, projection='3d')\r\n",
    "ax.title.set_text(\"Plot of the Franke Function (noise added)\")\r\n",
    "ax.view_init(elev=30., azim=-25.0)\r\n",
    "ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_zlabel(\"z\")\r\n",
    "surf = ax.plot_surface(x,y,z_noisy, cmap=cm.coolwarm, linewidth = 0, antialiased=False)\r\n",
    "plt.savefig(f\"{REPORT_FIGURES}franke_function_preview.pdf\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining and creating the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "n = 2000 # The number of point in the franke function\r\n",
    "x = np.random.rand(n)\r\n",
    "y = np.random.rand(n)\r\n",
    "z = FrankeFunction(x, y) \r\n",
    "noise = 0.05 * np.random.randn(n)\r\n",
    "z += noise # Adding noise to the function values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Approximate the franke function using ordinary least squares\r\n",
    "We estimate the franke functinon using polynomials up to 6th degree. We than look at the MSE scores to look for overfitting. We use the MSE score values from the test data to determine overfit together with the corveture of the plots"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot of fit for all degrees before diciding by evaluation on which degree that yeilds the best fit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def randrange(n, vmin, vmax):\r\n",
    "    return (vmax - vmin)*np.random.rand(n) + vmin\r\n",
    "\r\n",
    "fig = plt.figure(figsize=(8,8))\r\n",
    "degrees = 6\r\n",
    "z_train_OLS = pd.DataFrame()\r\n",
    "z_hat_train_OLS = pd.DataFrame()\r\n",
    "z_test_OLS = pd.DataFrame()\r\n",
    "z_hat_test_OLS = pd.DataFrame()\r\n",
    "\r\n",
    "# TODO: Must fix so that training and test data are used. \r\n",
    "# Must evalute model using MSE from traning and test\r\n",
    "for degree in range(1, degrees + 1):\r\n",
    "    X = create_X(x, y, degree) # Design Matrix\r\n",
    "    model = OLS() # The model\r\n",
    "    model.fit(X, z) # Fitting the model\r\n",
    "    z_hat = model.predict(X) # predict on train data\r\n",
    "    \r\n",
    "    # Plot\r\n",
    "    ax = fig.add_subplot(3,2, degree, projection='3d')\r\n",
    "    ax.view_init(elev=30., azim=-25.0)\r\n",
    "    ax.title.set_text(f\"OLS/Linear fit of degree{degree}\")\r\n",
    "    ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_zlabel(\"z\")\r\n",
    "    ax.scatter3D(y, x, z_hat, c=z_hat ,marker = '.', cmap=cm.coolwarm)\r\n",
    "fig.suptitle(\"OLS fit to the Franke Function\")\r\n",
    "plt.tight_layout()\r\n",
    "plt.savefig(f\"{REPORT_FIGURES}franke_function_fit_OLS.pdf\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def randrange(n, vmin, vmax):\r\n",
    "    return (vmax - vmin)*np.random.rand(n) + vmin\r\n",
    "\r\n",
    "fig = plt.figure(figsize=(8,8))\r\n",
    "degrees = 6\r\n",
    "z_train_OLS = pd.DataFrame()\r\n",
    "z_hat_train_OLS = pd.DataFrame()\r\n",
    "z_test_OLS = pd.DataFrame()\r\n",
    "z_hat_test_OLS = pd.DataFrame()\r\n",
    "\r\n",
    "# TODO: Must fix so that training and test data are used. \r\n",
    "# Must evalute model using MSE from traning and test\r\n",
    "for degree in range(1, degrees + 1):\r\n",
    "    X = create_X(x, y, degree) # Design Matrix\r\n",
    "    X_train, X_test, z_train, z_test = train_test_split(X,z, test_size=0.2, shuffle=True, random_state=4155)\r\n",
    "    \r\n",
    "    model = OLS() # The model\r\n",
    "    model.fit(X_train, z_train) # Fitting the model\r\n",
    "    \r\n",
    "    # Predictions\r\n",
    "    z_hat_train = model.predict(X_train) # predict on train data\r\n",
    "    z_hat_test = model.predict(X_test) # predict on test data\r\n",
    "\r\n",
    "    # Evaluatation metrics\r\n",
    "    # TODO:\r\n",
    "    # Filling up dataframes\r\n",
    "    z_train_OLS[degree] = z_train.flatten() \r\n",
    "    z_hat_train_OLS[degree] = z_hat_train.flatten()\r\n",
    "    z_test_OLS[degree] = z_test.flatten()\r\n",
    "    z_hat_test_OLS[degree] = z_hat_test.flatten()\r\n",
    "\r\n",
    "\r\n",
    "# MSE calculations for all lambda values\r\n",
    "mse_scores_train = ((z_train_OLS - z_hat_train_OLS) ** 2).mean()\r\n",
    "mse_scores_test = ((z_test_OLS - z_hat_test_OLS) ** 2).mean()\r\n",
    "# R2 calculations for all lambda values\r\n",
    "R2_scores_train = 1 - ((z_train_OLS - z_hat_train_OLS) ** 2).sum() / ((z_train_OLS - z_train_OLS.mean())**2).sum() \r\n",
    "R2_scores_test = 1 - ((z_test_OLS - z_hat_test_OLS) ** 2).sum() / ((z_test_OLS - z_test_OLS.mean())**2).sum()  \r\n",
    "\r\n",
    "# Plots\r\n",
    "plt.figure(figsize=(12,8))\r\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_train, label=\"Training data\")\r\n",
    "plt.plot(np.arange(1,degrees+1), mse_scores_test, label=\"Test data\")\r\n",
    "plt.xlabel(\"Model complexity - Number of polynomial degrees\")\r\n",
    "plt.ylabel(\"MSE\")\r\n",
    "plt.title(\"Training evaluation on OLS regression fit\")\r\n",
    "plt.legend()\r\n",
    "plt.tight_layout()\r\n",
    "plt.savefig(f\"{REPORT_FIGURES}Franke_Function_OLS_fit_evaluation.pdf.pdf\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "best_degree = 5\r\n",
    "X = create_X(x, y, best_degree) # Design Matrix\r\n",
    "model = OLS() # The model\r\n",
    "model.fit(X, z) # Fitting the model\r\n",
    "z_hat = model.predict(X) # predict on train data\r\n",
    "\r\n",
    "# Plot\r\n",
    "\r\n",
    "fig = plt.figure()#figsize=(8,8))\r\n",
    "ax = plt.axes(projection='3d')\r\n",
    "ax.title.set_text(f\"OLS regression fit to the Franke Function\\nbest degree {best_degree},\")\r\n",
    "#ax.view_init(elev=5., azim=85.0)\r\n",
    "ax.view_init(elev=30., azim=-25.0)\r\n",
    "ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_zlabel(\"z\")\r\n",
    "ax.scatter3D(y, x, z_hat, c=z_hat ,marker = '.', cmap=cm.coolwarm)\r\n",
    "plt.savefig(f\"{REPORT_FIGURES}franke_function_best_fit_OLS.pdf\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Approximate the franke function using Ridge regression\r\n",
    "First we find the optimal value for the lambda parameter by splitting the input data into training and test data. Than we fit the model using different values for lambda. For each lambda value we use the mean square error on the test data evaluate how god the fit is. The best lambda value is used to fit a new model with all data (not using train-test split). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we find the optimal parameter value of lambda, $\\lambda$, by evaluating overfit from evaluation plot where we plot the values for MSE for training and test data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "features = 6\r\n",
    "#x = np.arange(0, 1, 0.05)\r\n",
    "#y = np.arange(0, 1, 0.05)\r\n",
    "\r\n",
    "X = create_X(x, y, n=features) # Design Matrix\r\n",
    "z = FrankeFunction(x,y)\r\n",
    "z += noise\r\n",
    "nbf_lambdas = 300\r\n",
    "lambdas = np.logspace(-6,6, nbf_lambdas)\r\n",
    "z_train_ridge = pd.DataFrame()\r\n",
    "z_hat_train_ridge = pd.DataFrame()\r\n",
    "z_test_ridge = pd.DataFrame()\r\n",
    "z_hat_test_ridge = pd.DataFrame()\r\n",
    "\r\n",
    "for lam in lambdas:\r\n",
    "    X_train, X_test, z_train, z_test = train_test_split(X,z, test_size=0.2, shuffle=True)\r\n",
    "    model = RidgeRegression(lam) # The model\r\n",
    "    model.fit(X_train, z_train) # Fitting the model\r\n",
    "   \r\n",
    "    # Predictions\r\n",
    "    z_hat_train = model.predict(X_train) # predict on train data\r\n",
    "    z_hat_test = model.predict(X_test) # predict on test data\r\n",
    "\r\n",
    "    # Filling up dataframes\r\n",
    "    z_train_ridge[lam] = z_train.flatten() \r\n",
    "    z_hat_train_ridge[lam] = z_hat_train.flatten()\r\n",
    "    z_test_ridge[lam] = z_test.flatten()\r\n",
    "    z_hat_test_ridge[lam] = z_hat_test.flatten()\r\n",
    "\r\n",
    "\r\n",
    "# MSE calculations for all lambda values\r\n",
    "mse_scores_train = ((z_train_ridge - z_hat_train_ridge) ** 2).mean()\r\n",
    "mse_scores_test = ((z_test_ridge - z_hat_test_ridge) ** 2).mean()\r\n",
    "# R2 calculations for all lambda values\r\n",
    "R2_scores_train = 1 - ((z_train_ridge - z_hat_train_ridge) ** 2).sum() / ((z_train_ridge - z_train_ridge.mean())**2).sum() \r\n",
    "R2_scores_test = 1 - ((z_test_ridge - z_hat_test_ridge) ** 2).sum() / ((z_test_ridge - z_test_ridge.mean())**2).sum()  \r\n",
    "\r\n",
    "# Plots\r\n",
    "plt.figure(figsize=(12,8))\r\n",
    "plt.plot(-np.log(lambdas), mse_scores_train, label=\"Training data\")\r\n",
    "plt.plot(-np.log(lambdas), mse_scores_test, label=\"Test data\")\r\n",
    "#plt.plot(mse_scores_train,-np.log(lambdas), label=\"Training data\")\r\n",
    "#plt.plot(mse_scores_test,-np.log(lambdas), label=\"Test data\")\r\n",
    "plt.xlabel(\"log(lambda)\")\r\n",
    "plt.ylabel(\"MSE\")\r\n",
    "plt.title(\"Training evaluation on Ridge regression fit\")\r\n",
    "plt.legend()\r\n",
    "plt.savefig(f\"{REPORT_FIGURES}Franke_Function_Ridge_fit_evaluation.pdf\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-8-5c97b1ad3c48>:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  z_train_ridge[lam] = z_train.flatten()\n",
      "<ipython-input-8-5c97b1ad3c48>:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  z_hat_train_ridge[lam] = z_hat_train.flatten()\n",
      "<ipython-input-8-5c97b1ad3c48>:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  z_test_ridge[lam] = z_test.flatten()\n",
      "<ipython-input-8-5c97b1ad3c48>:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  z_hat_test_ridge[lam] = z_hat_test.flatten()\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "lam = -1.1\r\n",
    "model = RidgeRegression(lam) # The model\r\n",
    "model.fit(X, z) # Fitting the model\r\n",
    "\r\n",
    "# Predictions\r\n",
    "z_hat = model.predict(X) # predict on train data\r\n",
    "\r\n",
    "# Evaluatation metrics\r\n",
    "# TODO:\r\n",
    "results_df = pd.DataFrame(columns=[\"MSE\", \"R2-score\"], index=[\"Training data\", \"Test data\"])\r\n",
    "results_df[\"MSE\"] = MSE(z, z_hat)\r\n",
    "results_df[\"R2-score\"] = R2(z, z_hat)\r\n",
    "results_df.to_csv(f\"{REPORT_DATA}redge_reg_lambda_{lam}.csv\")\r\n",
    "\r\n",
    "# Plot\r\n",
    "fig = plt.figure()#figsize=(8,8))\r\n",
    "ax = plt.axes(projection='3d')\r\n",
    "ax.title.set_text(f\"Ridge regression fit to the Franke Function\\nDegree {degree},$\\lambda$:{lam}\")\r\n",
    "#ax.view_init(elev=5., azim=85.0)\r\n",
    "ax.view_init(elev=30., azim=-25.0)\r\n",
    "ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_zlabel(\"z\")\r\n",
    "ax.scatter3D(x,y,z,c=z, cmap=cm.coolwarm, marker = '.')\r\n",
    "plt.savefig(f\"{REPORT_FIGURES}franke_function_Ridge_fit.pdf\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('pytorch1': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "interpreter": {
   "hash": "4100b45d1ec7c24b8fe1569d39871ffb9fcc213dcc50046fc68fe247fbf6e84f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}